\chapter{Kernel CN:\ Grammar}%
\label{chap:kernel-grammar}

\kl{CN} and \kl{Kernel} CN have different grammars. This is because \kl{CN} is
intended to be used by C programmers, whereas \kl{Kernel} CN is more for type
theorists, and also to be more convenient to work with as a formalism. The
primary differences are (a) \kl{CN} is implemented over (a version of)
\kl{Core}, whereas the \kl{kernel} is defined over a let-normalised version of
\kl{Core} (b) \kl{CN}'s grammar of types is close to the surface syntax and so
each construct serves many purposes whereas the \kl{kernel}'s grammar of types
is more traditional and each construct only serves one purpose.

In this chapter I will present the relevant parts of \kl{CN}'s syntax of
predicate definitions and assertions, and the \kl{kernel}'s syntax of types and
relevant terms, with a particular focus on explicit resource terms.

\section{CN Syntax}%

\begin{figure*}[tp]
    \centering
    \includegraphics{figures/cn-grammar-1}
    \includegraphics{figures/cn-grammar-2}
    \includegraphics{figures/cn-grammar-3}
    \caption{Grammar of CN.}\label{fig:cn-grammar}
\end{figure*}

A file for \kl{CN} consists of series of top-level declarations of annotated C
functions, (separation logic) predicate definitions, (purely logical) function
definitions, and datatype declarations.\sidenote{Does the kernel formalism
support datatypes?} 

Function definitions for C introduce the identifiers for the arguments into the
scope of the pre- and postconditions, preceded by \cninline{requires} and
\cninline{ensures} respectively. Pre- and postconditions are a list of
`${conditions}$', each followed by a semi-colon.
\begin{itemize}
    \item \cninline{take id = resource} is a \kl{monadic} bind, which binds
        the \emph{output} arguments of the resource to the identifier \cninline{id},
        which doubles up as an assertion about the heap.
    \item \cninline{constraint} is a boolean-valued expression, which acts as a
        pure assertion.
    \item \cninline{let id = t} is simply an abbreviation for the expression
        \cninline{t} bound to \cninline{id}; it has no effect on the context.
\end{itemize}

\kl{CN} constraints (pure assertions) are either simple terms, or quantified
constraints.\sidenote{These must be manually instantiated by the user.}

\kl{CN} \kl{resource}s are simply a predicate \cninline{p(t1, .., tn)} % chktex 26 chktex 12 chktex 36
or an iterated predicate of the form
\cninline[breaklines]|each (<type> i; <guard>) { <pred>( array_shift(p i) ) }|. % chktex 36 chktex 37
Predicate names \cninline{p} are either
\cninline{Owned<ct>}/\cninline{Block<ct>}, representing ownership of an
initialised (read and write)/uninitialised location (a points-to $\mapsto{}$)
indexed by a C type, or \cninline{Alloc} representing an allocation, or a
user-defined one.

Similar to C syntax, \kl{CN} \kl{predicate} definitions first specify return
type, then a name, and a \intro{base type} annotated list of arguments. Their
definition consists of either a top-level \cninline{f} with a list of ${specs}$
in each branch, or a just the list of ${specs}$ at the top-level. ${specs}$ are
the ${conditions}$ followed by a return expression.

\section{Kernel CN types}

\begin{marginfigure}
    \centering
    \includegraphics{figures/kernel-fun-1}
    \includegraphics{figures/kernel-fun-2}
    \includegraphics{figures/kernel-ret}
    \caption{\kl{Kernel CN} function and return types.}\label{fig:kernel-fun-ret}
\end{marginfigure}

\cref{fig:kernel-fun-ret} show the grammar of function types ${fun}$ which
include both pre- and postconditions, and return types ${ret}$, which
represents postconditions. Where the \kl{CN} grammar pre- and postconditions
are a flats list of ${conditions}$, these types are nested and have quantifiers
to make explicit their scoping, specifically that the variables bound in the
precondition are available for use in the postcondition. This is necessary
because these types depend on computational values, and so, for example, a call
to a function needs to propagate the symbolic or concrete function arguments
into the rest of a pre- and postcondition (via a spine judgement).

In order, both include: quantification over computational (program) values,
quantification over logical (ghost) values, separating implication/conjunction
for assertions about the heap,\sidenote{Should I change this (back) to
$\otimes$ and $\multimap$? Or would that be confusing?} and logical
implication/conjunction for pure assertions (in a linear context, about an
empty heap).

Though this seems like a large departure from the syntax given in
\cref{fig:cn-grammar}, the mapping into this is straightforward, and helps
clarify what each \kl{CN} surface construct represents in more familiar,
type theoretic terms.

Before I explain this mapping, there is one more key component left to explain:
\emph{resource types}. As seen in~\cref{fig:kernel-res}, they are type for
separation logic assertions, which will be treated linearly. The constructs are
quite standard, except for the only mention of branching in any of the types,
with the \emph{ordered} disjunction, to prevent the need for backtracking,
rather than the usual $\vee{}$.

For ease of implementation and formalisation, we do not have branching for pre-
and postconditions and function/return types, but this does not affect
expressiveness because these can be embedded into the resource types. Note that
in the formalisation, the branching is not restricted to the top-level in
predicate definitions, but can occur directly in function types, and can be
arbitrarily nested.

Predicates ${pred}$ and quantified predicates ${qpred}$ are simple
${pred\_term}$ and ${qpred\_term}$ with output arguments, as seen
in~\cref{fig:kernel-qpred}. I will explain why I factored out ${pred\_term}$
and ${qpred\_term}$ later. For now, I will draw attention to the fact that
where there is a distinction between \cninline{Owned} and \cninline{Block} in
the surface syntax, the formalisation tracks whether or not a points-to has
been initialised in a symbolic \emph{record} field ${.init}$, whose type
mirrors the structure of the C type, i.e.\ with leaves which are booleans, and
branches which are records or arrays. This allows for more fine-grained control
over reading and writing \emph{partially} (and completely) uninitialised reads
of structs/unions,\sidenote{\url{https://www.cl.cam.ac.uk/~pes20/cerberus/notes98-2018-04-21-uninit-v4.html\#reads-of-partially-uninitialised-structsunions-as-a-whole} % chktex 8
Though honestly I could just get rid of this because it would be simpler.} as
allowed by the C standard.

\begin{marginfigure}
    \centering
    \includegraphics{figures/kernel-pred-name}
    \includegraphics{figures/kernel-qpred-term}
    \includegraphics{figures/kernel-qpred}
    \caption{\kl{Kernel CN} predicate and quantified predicate terms (without
        output arguments) and types (with output arguments). The formalisation
        is set up with some syntactic sugar (marked with $\mathsf{S}$) to make
        the meanings of these constructs more intuitive.}\label{fig:kernel-qpred}

\end{marginfigure}

\begin{marginfigure}
    \centering
    \includegraphics{figures/kernel-res-1}
    \includegraphics{figures/kernel-res-2}
    \caption{\kl{Kernel CN} resource types, a linear type for separation logic
        assertions.}\label{fig:kernel-res}
\end{marginfigure}

\section{Desugaring \kl{CN} types into \kl{kernel} types}\label{sec:desugaring}

All judgements in the formalisation which have a natural bidirectional
interpretation have the information they are synthesising highlighted with
\colorbox{pink!30}{light pink} background. The grammars presented below will
refer to ${term}$, ${iguard}$, ${ptr}$, ${init}$, ${value}$, ${iarg}$, ${oarg}$
(and later, ${alloc}$). These are all simply aliases for pure (SMT) terms, used
so that the roles of these terms in different productions, especially ones
productions which refer to multiple instances of them, are clearer. \intro{Base
types}, represented by $\beta$ or ${bty}$ are simply they types of such terms.

Desugaring \kl{CN} types into \kl{Kernel CN} starts with the C functions, which
map their parameters into computational variables in function types, or with
predicate definitions, which map their parameters into essentially a
$\lambda$-abstraction over a resource type (\cref{fig:prepost-to-kernel}). To
clear, this is merely quantifying over pure (SMT) terms in the type, rather
than any higher-order assertions about the shape of the heap.

\begin{figure*}[tp]
    \includegraphics{figures/prepost-to-kernel}
    \caption{\kl{CN} to \kl{Kernel CN} pre- and postcondition and predicate
        definition desugaring. For the C functions, each C argument is bound to
        a computational argument, with a base type corresponding to the C type.
        Predicate definitions simply abstract pure (SMT) terms over
        resources.}\label{fig:prepost-to-kernel}
\end{figure*}

Desugaring preconditions into function types requires the postcondition to be
desugared first; because that is similar to precondition desugaring, I will
omit it for space. Abbreviations are simply substituted into the function
type.\sidenote{In the implementation, for better error messages, they are bound
to a fresh variable and constrained with an equality constraint.}\label{sn:abbrev}
Constraints are mapped into logical implications. The formalisation can handle
ifs directly in the precondition, unlike the surface syntax which allows ifs to be placed
only at the top-level of a predicate (\cref{sec:restriction-branching}).

The monadic binding \cninline{take id = ..} is always translated into a logical % chktex 26
quantification over the output argument of the (quantified)\sidenote{Should
really consider renaming these to `iterated predicates'.} predicates. Because
all the predicate definitions are guaranteed to be precise, all the logical
quantifications can be inferred when required.

\begin{figure*}[tp]
    \includegraphics{figures/preconditions-to-kernel-1}
    \includegraphics{figures/preconditions-to-kernel-2}
    \caption{\kl{CN} to \kl{Kernel CN} precondition desugaring.
        Postcondition desugaring is similar, and thus omitted.}\label{fig:precond-to-kernel}
\end{figure*}

Desugaring predicate definitions is similar. Because the grammar is used in two
contexts, inside a pre- or postcondition where a return is not allowed, and in
a predicate definition where a return is allowed, the desugaring is
parameterised over whether or not a return is expected. If one is not, then the
resource is simply an $\mathsf{emp}$, otherwise it is an equality constraint,
as shown in \cref{fig:monad-sl}. Outputs of (quantified) predicates are always
assumed to be of the shape of a record for uniformity, which I will justify
later. Abbreviation are also substituted in,\sidenote{As in note~\ref{sn:abbrev}.}
Because pure (SMT) terms are syntactically stratified out of
the impure ones, they are embeded directly into a resource type (separation
logic assertion) with $\astRef{}$. Ifs in the syntax are translated into ifs in
the resource type grammar, with some adjustment based on whether it occurs in a
terminal place; no returns are expected/allowed in non-terminal
ifs. This restriction side steps the need for a more complicated
destination-passing style \sidecite{shaikhha2017destination} transformation to
translate the semantics early-returns into precise separation logic
assertions. Lastly, as before, the monadic binding \cninline{take id = ..} is % chktex 26
always translated into a logical quantification over the output argument of the
(quantified) predicate.

\begin{figure*}[tp]
    \includegraphics{figures/predicate-to-kernel-1}
    \includegraphics{figures/predicate-to-kernel-2}
    \caption{\kl{CN} to \kl{Kernel CN} user-defined predicate desugaring.}\label{fig:pred-to-kernel}
\end{figure*}

\section{Let-normal Core}

Not only is the formalisation defined over a desugar representation of types
from the surface \kl{CN} syntax, it is also defined over the let-normal form of
the \kl{Core} grammar
(\cref{fig:pure-core-grammar,fig:effectful-core-grammar}). By let-normal, I
mean an A-normal\sidecite{flanagan1993essence} form which is closed under
substitution.

Specifically, it syntactically stratifies values and
expressions from which we would like to \emph{synthesise} type information, and
top-level values and expressions against which we would like to \emph{check} a
given type. This applies to both pure and effectful fragments, leading to a
four-fold distinction in the let-normal form of the grammar.

This dramatically simplifies presenting and working with the type system, because:
\begin{itemize}
    \item \coreinline{undef()}. In typing this, it is necessary to give is a % chktex 36
        checking rule, since control flow is required to not reach that point
        (\cref:fig:core-ub-typing).
    \item \textbf{Control flow}. Here too, we would also very much like to use
        a checking rule, since this would alleviate the need to construct
        \emph{join}-points in types (or require users to place annotations
        after all \cinline{if}-statements).
    \item \textbf{Lets}. In typing these, we would also very much like to use a
        \emph{synthesis} rule for the bound expression, since that removes the
        need for an annotation to be placed on the binder there.
    \item \textbf{Memory actions}. These too are well-suited to synthesis, since
        they can manipulate they manipulate the resource context via the
        resources types they will synthesise.
\end{itemize}

Initial versions of \kl{CN} did a full A-normalisation of Core, but this resulted
in far logical variables being created (one for each intermediate sub-expression)
and these were very difficult to relate back to the source program in concrete
counter-examples produced by the SMT solver.\sidenote{Location information was not
tracked properly either so this was doomed.} Hence, this was
removed.\sidenote{\href{https://github.com/rems-project/cerberus/commit/21808139bda2ee320756c71eb22dbd57d0986f97}{Commit 21808139.}}.
The way that \kl{CN} currently manages the flow of information is by explicitly
passing around continuations; when it comes across any expression which is
treated as a top-level one in \kl{let-normal Core}, it simply does the appropriate
checks and then \emph{drops} the continuation.\sidenote{\href{https://github.com/rems-project/cerberus/commit/350fefc675626dcc69c7adc9edea30ff9687b752}{Commit 350fefc6.}}
This makes the code more fragile but saves the need for maintaining another large
syntax tree which needs pretty-printers, debug printers, source location
mappings and so on.

Since it would require more complex fractional permissions, I left out
let-normalising \coreinline{unseq()}, \coreinline{let weak} and other % chktex 36
constructs related to C's loose evaluation order. However, this also means
the formalisation glosses over the fact that those constructs can contain each
other in a semantically meaningful way, such that flattening out that nesting
seems impossible. The solution would be to require type-annotations on any
situation which requires top-level expressions to be nested inside one another.
Indeed, as we shall see in \nameref{chap:kernel-soundness}, annotations on
nested top-level expressions are required anyway for proving type preservation
for function calls to pure Core functions, effectful Core procedures, and
elaborated C functions. And as I demonstrate in \nameref{chap:kernel-alternative},
it seems very difficult to avoid some sort of normalisation somewhere in
the type system in the presence of early returns in sub-expressions.

For now, I will confine my discussion of the let-normal grammar to top-level
expressions (full details for both nested and top-level expressions are
available in the appendix). Pure expressions include things such as pure values,
datatype values, pointer arithmetic for arrays (in \kl{de facto}, not \kl{ISO})
and struct/union members, boolean negation, binary operations and relations, function
calls and assertions. Top-level values and expressions are in
\cref{fig:kernel-tp}. As mentioned earlier, constructs where control flow
should not reach, such as \coreinline{undef()} must be in a checking % chktex 36
judgement, so top-level values consists of them and regular pure values lifted
to the top.

\begin{marginfigure}
    \includegraphics{figures/kernel-tpval}
    \includegraphics{figures/kernel-tpexpr}
    \caption{Top-level pure values and expressions in let-normal Core.}\label{fig:kernel-tp}
\end{marginfigure}

Top-level effectful values are the same as top-level pure values.
Effectful values and expressions are further split into \intro{sequenced}
expressions and \intro{indeterminately} sequenced expressions. Sequenced
expression include only C function calls and Core procedure calls.
As shown in \cref{fig:kernel-is-expr}, \kl{indeterminately} sequenced
expressions include annotated top-level values, memory operations, and explicit
terms to pack or unpack predicates (I will explain explicit resource terms in
\cref{subsec:res-terms}).

\begin{marginfigure}
    \includegraphics{figures/kernel-is-expr-1}
    \includegraphics{figures/kernel-is-expr-2}
    \caption{\kl{Indeterminately} sequenced expressions in let-normal
        Core.}\label{fig:kernel-is-expr}
\end{marginfigure}


Sequenced top-level effectful expressions (\cref{fig:kernel-texpr}) are
simply the constructs which mention a pure expression inside them; you can see
that the bound expression in a \coreinline{let}, the scrutinee of a
\coreinline{case}, the condition of an \coreinline{if}, as well as the
arguments to \coreinline{run} are contain pure expressions
(\cref{fig:effectful-core-grammar}).

However, this are mutually defined with \kl{indeterminately} sequenced
top-level expressions (\coreinline{let weak} and \coreinline{let strong}), for
which as I mentioned before, I do not have accurate support. This is mutual
recursion is achieved the $\mathit{texpr}$ production.

\begin{marginfigure}
    \includegraphics{figures/kernel-seq-texpr}
    \includegraphics{figures/kernel-texpr}
    \caption{Top-level expressions in let-normal Core.}\label{fig:kernel-texpr}
\end{marginfigure}

\subsection{Resource terms}\label{subsec:res-terms}

\subsection{Heap types}\label{subsec:heap-types}

\subsection{Resource Terms, Quantifier Inference}\label{subsec:resq-inf}

Explain \intro{bidirectional} (for quantifiers), linear resources, constraints.
Explicit witness to having permissions, which are linearly typed (just ingredients).
Explain enough rules \textemdash{} typing, operations, especially the weird heaps.
And of course, type safety statement and its proof.
Linear terms in a refinement type system.

(Dep ML, L3, F star, Jhala, ATS)

Different and unusual compared to Iris style \textemdash{} separation proofs outside the program.

Proof term in one sense, but also factors out operations for resource manipulation.

\subsection{Heaps}


\subsection{Type Safety}

\chapter{Kernel CN:\ Typing rules}%

\chapter{Kernel CN:\ Proof of soundness}%
\label{chap:kernel-soundness}

Weird heaps.

\chapter{Informing implementation discussions}%

In the early stages, \kl{CN} was implemented by Christopher Pulte and Thomas
Sewell, based on sketches by Neel Krishnaswami. I started formalising
\kl{Kernel CN} much later, and benefited by the clarity of having an
implementations and implementers which and whom I could refer to in moments of
confusion.

However, this mode of development means that there were \emph{many} design
decisions made in a rather conservative context, because the programming was
always of a system which was being defined along the way, rather than a
well-understood pre-existing one. Extensions to syntax and inference were
always the minimum required for verifying the pKVM buddy allocator, lest
performance and inference suffer greatly, rather than ones based on a strong
formal and holistic consideration of the constructs and interactions at play.

As such, there are several restrictions in the implementation, which with the
benefit of hindsight and formalism, are completely unnecessary, but persist as
technical debt. This chapter list a few of these, and explains how the
formalisation brings much needed clarity to many questions around the
implementation.

\url{https://github.com/rems-project/cerberus/labels/language}
\url{https://github.com/rems-project/cerberus/labels/resource\%20reasoning}

\section{Supporting partially initialised reads of structs/unions}

This is not asked for, and actually seems to add a non-trivial amount of noise
and book-keeping to the formalisation. This suggests that the feature is not
worth implementing in CN unless a strong use-case comes up.

\section{Auto unfolding scheme for logical functions}
\url{https://github.com/rems-project/cerberus/issues/483}

\section{Higher-order resources}
\url{https://github.com/rems-project/cerberus/issues/483}

\section{Restrictions on branching}\label{sec:restriction-branching}
\url{https://github.com/rems-project/cerberus/issues/483}
\url{https://github.com/rems-project/cerberus/issues/266}

\section{Removing the pointer first restriction on predicates}
\url{https://github.com/rems-project/cerberus/issues/303}

\section{Unifying the syntax of functions, predicates and specifications}
\url{https://github.com/rems-project/cerberus/issues/304}


\chapter{An alternative presentation}\label{chap:kernel-alternative}

Perhaps a short chapter about MiniCN\@? This could demonstrate the strong
advantages of defining a type system over a first-order functional language,
rather than trying to do so directly over something C-like.

It would also give some space to the interesting but yet-to-be-baked ideas
from the Fuliminate paper.

