\bookmarksetup{startatroot}

\chapter{Future Directions}%
\label{chap:future-directions}

\margintoc{}

In this chapter, I will recap some of \kl{CN}'s more pressing limitations, and
sketch out some solutions to it.

\section{Inferring frames}

Recall the \kl{Core} operators for expressing most control-flow:
\coreinline{run()} and \coreinline{save()}, first presented % chktex 36
in~\nameref{sec:core-grammar}.

These very general constructs are used by \kl{Cerberus}' elaboration into
\kl{Core} to support a variety of control flow constructs: loops (continue,
break, iteration), switches (cases, fall-through and defaults), labels, and
\cinline{goto}.

\kl{CN} currently deals with these on an ad-hoc basis, such as by marking
labels with their original structure and use (loop continue, loop break,
return, switch, case, default) labels, and maximally inlining label bodies to
avoid requiring annotations at awkward places such as loop continues and breaks
and switch cases.

There are two main problems with this approach.

\begin{enumerate}
    \item \textbf{Inlining slows down performance and duplicates
        work.}\sidenote{\href{https://github.com/rems-project/cerberus/issues/289}{Cerberus\#289.}}
        The transformation from \kl{Core} with \coreinline{run()}  % chktex 36
        and \coreinline{save()}to \kl{Core} with non-recursive runs % chktex 36
        inlined and recursive runs hoisted to the top level induces some
        impressive code bloat. A nested-loop
        example,\sidenote{\href{https://github.com/rems-project/cerberus/issues/938}{Cerberus\#938.}}
        shows a 7x increase (226 lines of \kl{Core} to 1389 lines); a
        \cinline{switch} with 3 \cinline{case}s and a \cinline{default}
        example, shows a 5x increase.
    \item \textbf{Loop-annotations do not compose as expected, and are thus confusing and
        verbose.}\sidenote{\href{https://github.com/rems-project/cerberus/issues/913}{Cerberus\#931.}}
\end{enumerate}

The reason for this is the \kl{Core} dynamics for the
\coreinline{run()} operator. In particular, note that the current % chktex 36
continuation $C$ is discarded, and the continuation associated with the label
$\cnnt{id}$, $C_\cnnt{id}$ is resumed.

{\small%
\[
\inferrule[{[Run]}]
  { \mathrm{labelmap} ( \cnnt{id} ) = \left(\cncomp{x_i}{i}\right).\ C_{\cnnt{id}} [ E_{\cnnt{id}} ] \\
    \cncomp{e_i \Downarrow \cnnt{value}_i}{i} }
  { \langle h , C\left[ \cnkw{run}\, \cnnt{id} \left( \cncomp{e_i}{i} \right) \right] , \kappa \rangle
    \rightarrow \left< h , C_{\cnnt{id}} \left[ \left[ \cncomp{\cnnt{value}_i / x_i}{i} \right] E_{\cnnt{id}} \right] , \kappa \right> }
\]}

Its associated (\kl{ResCore}) typing rule is as follows. Note that because
control-flow does not return to this point in the program, the return type is
$\cnkw{false} \wedge \cnkw{I}$, even if, for example, a program is entering and
exiting a loop in a well-bracketed way. This has the effect of requiring the
pre-condition of the label to consume \emph{all} resources from the context
of a \coreinline{run()}. % chktex 36

{\small%
\[
\cndruleStmtXXRun{}
\]}

% \onlyUseRules{\cndefnStmt{}}{
%     \cndruleStmtXXRun{}
% }

In particular, this means that loop invariants must explicitly mention all
resources in a context (even ones which are morally framed out), so that they
are reinstated upon loop exit. The situation compounds for each level of
loop nesting: invariants of all enclosing loops must be manually threaded through
by the user
(see~\href{https://github.com/rems-project/cerberus/issues/913}{Cerberus\#931}).
The problem holds for all \kl{Core} labels, but only manifests itself at loops
since \kl{CN} inlines all labels whose bodies do not recursively \coreinline{run()} % chktex 36
to themselves.

Whilst it may be possible to engineer \kl{CN} for the common cases,
for example by extending \kl{Core} with specific loop or switch constructs,
the root of the problem will remain to handle labels and \cinline{goto}s in C.

Hence a prinicipled solution is desirable, and would be useful to ensure
our handling of the common cases is sound.

A \emph{potential} solution is to adjust the grammar and elaboration of
\kl{Core} to borrow an idea from an inductive representation of SSA, based on
work by~\sidetextcite{ghalayini2024denotational}: represent
\emph{dominance-based scoping} as \emph{lexical scoping}.

\begin{quote}
In particular, a variable $x$ is considered to be in scope at a specific point
$P$ if and only if all execution paths from the programâ€™s entry point to $P$
pass through a definition $D$ for $x$. In this case, we say that the definition
$D$ \emph{strictly dominates} $P$. The relation on basic blocks ``$A$ strictly
dominates $B$'' intersected with ``$A$ is a direct predecessor of $B$'' forms a
tree called the \emph{dominance tree} of the [control-flow graph].
\end{quote}

More specifically: replace the $\cnkw{save}\ \cnnt{id}\ (x {:}{=}\ \cnnt{pce}) \cnkw{in}\ E$
construct with a ``where-block'':
\[
    \cnnt{E}\ \cnkw{where}\ \left( \cncomp{\cnnt{id}_i\ (x_i {:} \beta_i).\ E_i}{i} \right).
\]
Labels $\cnnt{id}_i$ are
in scope in $E$ and in each $E_i$, allowing for mutual recursion, but are
encapsulated from the enclosing expression. This is unlike \kl{Core}'s
\coreinline{save()} operator, whose labels are in scope of the % chktex 36
enclosing \emph{procedure}. Variables defined in $E$ are not in scope in
$E_i$, which are parameterised over (for simplicity) a single variable $x_i$.
Most importantly, the enclosing expression is its \emph{frame}, and unlike
in \kl{Core}, is preserved in the dynamic semantics.

{\small%
\[
\inferrule[{[OpStmtWhere]}]
  { l' \eqdef \cncomp{\cnnt{id}_i\ (x_i {:} \beta_i).\ E_i}{i} \Colon{} l }
  { \langle h , E\ \cnkw{where}
     		\ ( \cncomp{\cnnt{id}_i\ (x_i {:} \beta_i).\ E_i}{i} ), l, \kappa \rangle
    \rightarrow \left< h , E , l', \kappa \right> }
\]}

{\small%
\[
\inferrule[{[OpStmtRunW]}]
  { l \eqdef l_1 \Colon \ldots \Colon l_k \Colon l'
    \and \cnnt{id}\ (x {:} \beta).\ E \in l_k
    \and \cnnt{pce} \Downarrow \cnnt{value} }
  { \langle h , \cnkw{run}\, \cnnt{id}\,\cnnt{pce}, l, \kappa \rangle
    \rightarrow \left< h , \left[ \cnnt{value} / x \right] E , l_k \Colon l', \kappa \right> }
\]}

The \kl{Core} thread-local reduction configuration is extended with a
\emph{stack} $l$, where each element is a family of label definitions
($\cncomp{\cnnt{id}_i\ (x_i {:} \beta_i).\ E_i}{i}$). Upon entry, the label
definitions are pushed ($\Colon$) on to the stack. When $\cnkw{run}$ning to a
label, we search the stack ($l \eqdef l_1 \Colon \ldots \Colon{} l_k \Colon
l'$) for it ($\cnnt{id}\ (x {:} \beta).\ E \in l_k$) and jump to its body,
proceeding with only the labels at and below that level on the stack ($l_k
\Colon{} l'$). This $l$ stack is cleared when returning from a
procedure.

As for typing, I conjecture the $\cnkw{where}$-stack in the dynamics would be
mirrored in an \emph{ordered} resource context $\mathcal{W}$, consisting of
linear resource contexts $\mathcal{R}$, separated by label definitions $l$ as
ordering markers. Typing these definitions requires threading such a
$\mathcal{W}$ through.

{\small%
\[
\inferrule[{[WhereDefns]}]
  { \cncomp{\cnnt{fun}_i \rightsquigarrow \mathcal{C}_i; \mathcal{L}_i; \Phi_i; \mathcal{R}_i}{i}
    \\ \cncomp{\mathcal{C}, \mathcal{C}_i; \mathcal{L}, \mathcal{L}_i; \Phi, \Phi_i; \mathcal{R}_i \Colon \mathcal{W} \vdash E_i \Leftarrow \cnnt{ret}}{i} }
  { \mathcal{C}; \mathcal{L}; \Phi; \mathcal{W} \vdash \cncomp{\cnnt{id}_i {:} \cnnt{fun}_i.\ E_i}{i} \Leftarrow \cnnt{ret} }
\]}

Typing a $\cnkw{where}$-block itself would pass any resources it has to the
left expression, and adding the definitions as an ordering marker to bring them
in scope. The same is done for checking the definition bodies too, because
labels can refer to each other (mutually) recursively.

{\small%
\[
\inferrule[{[StmtWhere]}]
  { \mathcal{C}; \mathcal{L}; \Phi; \mathcal{R} \Colon l \Colon \mathcal{W} \vdash E \Leftarrow \cnnt{ret}
    \and \mathcal{C}; \mathcal{L}; \Phi; l \Colon \mathcal{W} \vdash l \Leftarrow \cnnt{ret} }
  { \mathcal{C}; \mathcal{L}; \Phi; \mathcal{R} \Colon \mathcal{W} \vdash{} E\ \cnkw{where}\  l \Leftarrow \cnnt{ret} }
\]}

With this, I can sketch out a rough idea on typing for a $\cnkw{run}$ to any
label in scope. First any resources in the intervening depths would need to
disposed, for example, removing the storage for block-local variables. After
that, the $\cnnt{spine}$ would be checked against the function type
$\cnnt{fun}$ (obtained via a lookup of the label $\cnnt{id}$ in the ordered
resource context). Only the resources at the same level as the target
$\cnkw{where}$-block need to be consumed, everything above it is `framed' when
checking the label body.

{\small%
\[
\inferrule[{[StmtRun]}]
  { \mathcal{W} \eqdef \mathcal{R}_1 \Colon l_1 \Colon \ldots \Colon \mathcal{R}_k \Colon l_k \Colon \ldots \Colon \mathcal{R}_n \Colon l_n
    \and \cnnt{id} {:} \cnnt{fun}.\ E' \in l_k
    \\ \mathcal{C}; \mathcal{L}; \Phi; \mathcal{R}_1 , \ldots, \mathcal{R}_{k-1} \vdash E \Leftarrow \Sigma y{:} \cnkw{unit}.\ \cnkw{I}
    \\ \mathcal{C} ; \mathcal{L} ; \Phi ; \mathcal{R}_k \vdash \cnnt{spine} \Colon \cnnt{fun} >\!> \outpol{\sigma}; \outpol{\cnkw{false} \wedge \cnkw{I}} }
    { \mathcal{C}; \mathcal{L}; \Phi; \mathcal{W} \vdash{} \cnkw{let\ strong\ ()\ =}\ E\ \cnkw{in}\ \cnkw{run}\ \cnnt{id}\,\cnnt{spine} \Leftarrow \cnkw{false} \wedge \cnkw{I} }
\]}

As in~\arefpart{formalisation}, the aboves rules are phrased over a
hypothetical, explicitly annotated \kl{ResCore} with this new construct. Whilst
the setup enables inferring frames for a label, inferring pre-conditions
$\cnnt{fun}$ remains a distinct problem. As a first approximation, loops and
mutually recursive labels would require annotations from the user, though
annotations on loops would no longer need to manually thread through unusued
parts of the resource context. Other labels could have their pre-conditions
inferred by saving the typing context when the first $\cnkw{run}$ is
encountered, and ensuring that subsequent ones match the first, thus preventing
inlining and code bloat.

\section{Weak sequencing}

Sequencing strength is one of the most subtle and confusing aspects about C,
and its treatment in \kl{Core}, whilst inventive, is difficult to understand
and reason about.

Sequencing strength is used to specify the (lack of) ordering between memory
actions. Quoting at length from \sidetextcite{memarian2022cerberus}:

\begin{quote}
 Each use of an action is either ``positive'' (the default case) or
 ``negative'' (if the action appears in the syntax as the operand of the
 $\cnkw{neg()}$ operator). [..] Intuitively, a \kl{Core} action is negative
 when it elaborates what the C standard calls a ``side-effect'' (as opposed to
 value computations), that is, memory accesses which are not directly used for
 producing the value of a C expression. This is for example, the store
 performed by a postfix increment operator.
\end{quote}

The $\cnkw{let}\ \cnkw{strong}\ \cnnt{pat}\ \cnkw{=}\ \cnnt{E}_1\ \cnkw{in}\
\cnnt{E}_2$ construct sequences all memory actions performed by $\cnnt{E}_1$
before those performed by $\cnnt{E}_2$. However, $\cnkw{let}\ \cnkw{weak}$ only
sequences positive memory actions similarly; in other words, for $\cnkw{let}\
\cnkw{weak}\ \cnnt{pat}\ \cnkw{=}\ \cnnt{E}_1\ \cnkw{in}\ \cnnt{E}_2$, any
negative memory actions performed by $\cnnt{E}_1$ are unsequenced with respect
to all memory actions performed by $\cnnt{E}_2$. A race only occurs if
unsequenced memory actions have overlapping footprints, and is deemed \kl{UB}.

The formalisation and implementation of weak sequencing is complex. Here, I
offer a simpler presentation which I conjecture is equivalent and then offer
few remarks on how to adapt the \kl{CN} resource framework to fit it.

A footprint $\cnnt{fp}$ of an action is the range of bytes in memory it touches.
Like actions, footprints can be positive ${\color{red}\cnkw{pos}(\cnnt{fp})}$
or negative ${\color{red}\cnkw{neg}(\cnnt{fp})}$. A set of such footprints is
called an annotation ${\color{red}A}$, and I use the notation
${\color{red}A^{-}}$ to denote all the negative footprints in ${\color{red}A}$
and ${\color{red}A^{+}}$ for all the positive ones.

Negative actions step to a new $\cnkw{delay}(\cnnt{actions}, \cnnt{E})$
construct, where $\cnnt{actions}$ is a set.

{\small%
\[
\inferrule[{[ActionNeg]}]
    { \sigma \overset{\cnnt{action}}{\longrightarrow} \langle \_ , \cnkw{Unit}, \cnnt{fp} \rangle }
    { \langle \sigma, \cnkw{neg}(\cnnt{action}), \kappa \rangle \rightarrow
      \langle
          \sigma,
          \cnkw{delay}(\{\, \cnnt{action} \, \},
            \prescript{\color{red}\{\cnkw{neg}(\cnnt{fp})\}}{}{\cnkw{pure}(\cnkw{Unit})}),
          \kappa
      \rangle }
\]}

The dynamics of this construct allowing reducing either one of the actions in
the set, or the expression inside at any time; the latter because it counts as
a context $\cnnt{C} \, {:}{=} \, \ldots \mid \cnkw{delay}(\cnnt{actions}, C)$.
All $\cnkw{delay}$ed actions are negative, and all negative actions evaluate to
$\cnkw{Unit}$ (since they are side effects) and so the result of the action is
ignored. The footprint is unnecessary as per the previous rule.

{\small%
\[
\inferrule[{[DelayAction]}]
    { \sigma \overset{\cnnt{action}}{\longrightarrow} \langle \sigma' , \cnkw{Unit}, \_ \rangle }
    { \langle
          \sigma,
            \cnkw{delay}(\cnnt{actions} \cup \{\, \cnnt{action} \,\}, E),
          \kappa
      \rangle
      \rightarrow
      \langle \sigma', \cnkw{delay}(\cnnt{actions}, E), \kappa \rangle }
\]}

The next ingredient is a $\cnkw{disj}({\color{red}A}, E)$ construct, which is
used to check if ${\color{red}A}$ races with $E$.

{\small%
\[
\inferrule[{[Disj]}]
    { {\color{red}\neg \mathrm{do\_race}(A_1, A_2)} }
    { \langle
          \sigma,
          \cnkw{disj}({\color{red}A_1}, \prescript{\color{red}A_2}{}{\cnkw{pure}(v)}),
          \kappa
      \rangle
      \rightarrow
      \langle
          \sigma,
          \prescript{\color{red}A_1 \cup A_2}{}{\cnkw{pure}(v)},
          \kappa
      \rangle }
\]}

The construct is used for weak sequencing, and is also a context $\cnnt{C} \,
{:}{=} \, \ldots \mid \cnkw{disj}({\color{red}A}, C)$, allowing for the
expression inside to reduce. Splitting the annotations, placing the positive
ones on the outside and the negative ones in a $\cnkw{disj}$ ensures the
negative actions which \emph{were} part of evaluating $\cnkw{pure}(v)$ are
treated as if unsequenced with $E$, i.e.\ checked to ensure they do not race.

{\small%
\[
\inferrule[{[Weak]}]
    {  }
    { \langle
          \sigma,
          \cnkw{let}\ \cnkw{weak}\ \cnnt{pat}\ =\ \prescript{\color{red}A}{}{\cnkw{pure}(v)}\ \cnkw{in}\ E,
          \kappa
      \rangle
      \rightarrow
      \langle
          \sigma,
          \prescript{\color{red}A^{+}}{}{\cnkw{disj}({\color{red}A^{-}}, [ v / \cnnt{pat} ] E)},
          \kappa
      \rangle }
\]}

Nested $\cnkw{delay}$s can be combined by taking the union of the actions.

{\small%
\[
\inferrule[{[HoistDelay]}]
    {  }
    { \langle \sigma, \cnkw{delay}(\cnnt{actions}_1, \cnkw{delay}(\cnnt{actions}_2, E)), \kappa \rangle
      \rightarrow
      \\ \quad
      \langle \sigma, \cnkw{delay}(\cnnt{actions}_1 \cup \cnnt{actions}_2, E), \kappa \rangle }
\]}

Cruicially, $\cnkw{delay}$s can be hoisted out of $\cnkw{let}\ \cnkw{weak}$,
$\cnkw{unseq}$, $\cnkw{disj}$ and annotated $\prescript{\color{red}A}{}{E}$
expressions. The rule for the first is given below, and the rest follow a
similar pattern.

{\small%
\[
\inferrule[{[HoistWeak]}]
    {  }
    { \langle
          \sigma,
          \cnkw{let}\ \cnkw{weak}\ \cnnt{pat}\ =\ \cnkw{delay}(\cnnt{actions}, E_1)\ \cnkw{in}\ E_2,
          \kappa
      \rangle
      \rightarrow
      \\ \quad
      \langle
          \sigma,
          \cnkw{delay}(\cnnt{actions}, \cnkw{let}\ \cnkw{weak}\ \cnnt{pat}\ =\ E_1\ \cnkw{in}\ E_2),
          \kappa
      \rangle }
\]}

Notably absent, $\cnkw{let}\ \cnkw{strong}$ or $\cnkw{bound}$ expressions
\emph{do not} permit hoisting $\cnkw{delay}$s out of them. This forces any
delayed actions to be evaluated and any races to checked before proceeding.

Though unusual, the constructs and rules are simple. One straightforward
approach to typing could be to split resource types into positive and negative.
Negative resources are not usable, and are converted into positive ones after a
$\cnkw{let}\ \cnkw{strong}$ or a $\cnkw{bound}$ expression.

\section{Join-points}

\section{Higher order predicates}

\section{Integers and bitvectors}

\section{Predicate definition checks}

\chapter{Conclusion}%
\label{chap:conclusion}

\margintoc{}

In this thesis, I argued that building a verification tool for C, suitable
for handling low-level systems programming idioms, is two parts engineering,
and one part theory.

In~\arefpart{formalisation}, I discussed the origin of \kl{CN} as a tool
to verify the \kl{pKVM} hypervisor, and \kl{CN}'s design goal of lowering the
cost of verification and being usable by `systems programmers who know
Haskell'. I showed how these engineering constraints informed the choice of
\kl{CN}'s many theoretical foundations. I showed that those these foundations
are well-established and individually well understood, its application to a
large calculus \kl{Core} with unique features presents new challenges and
solutions, such as closely linking resource contexts to heaps by representing
them as separation logic predicates, and using explicit proof terms to encode
transformations on both. I concluded by feeding back insights from the theory
into actual engineering questions we face in developing and extending \kl{CN}.

In~\arefpart{mem-model}, I described a complex, formal, memory object
model for C which accurately captures most of the behaviour exhibited by
existing optimsing compilers. I showed that we can simplify some of the
complexity by exploiting the fact that in verification, small and
easy-to-explain changes to the code are justified if they make the typing rules
easier to implement and explain. These engineering constraints defined a
flexible perimeter for the design space, which I decomposed, guided by the
theory of the model's definitions, features, purposes, and representations in
the \kl{CN} type system. I showed that there are multiple reasonable design
points for typing rules to capture, and that doing so soundly can come down to
very subtle technical details. After summarising the point I chose for
\kl{CN-VIP}, I showed that factoring out the definition and usage of the heap
allows for significantly more modular updates and structure to the
overall proof of soundness. I then explained how I implemented these rules in
\kl{CN} and their effect on the annotation and performance overhead of \kl{CN}.
At the end, I discussed how I fed back insights from the theory to improve performance,
and outlined the development of new features such as support for lemmas and
better foundations for \kl{CN}.

Lastly, in~\arefpart{engineering}, I showed that a verification tool for
real-world C, needs some way of taming the complexity of large repositories and
non-standard or unsupported aspects of C. I showed that whilst \kl{CN}'s first
sub-goal of proving \kl{pKVM} seems to be feasible, there are several concrete
ways in which needs to become more usable. I showed that the second sub-goal of
on-going proof maintenance still needs substantial work. I then compared
\kl{CN} to other proof tools by discussing my experience on verifying a small
but informative example in all of them, which indicated where it stands with
respect to the academic state-of-the-art. I then discussed industry feedback on
\kl{CN}, which indicated where \kl{CN} stands in relation to user
expectations. Lastly, I synthesised the large gap between them by outlining
concrete expectations and how to achieve them for \kl{CN} and similar tools.

There are of course, longer-term lines of research to pursue based on \kl{CN}.
I see at least four broad directions for this. I already covered in detail the
direction of more trustworthiness in \nameref{sec:better-foundations}; I will
discuss the remaining aspects here.

\section{More expressiveness}

% Increasing expressiveness of \kl{CN} means adding support for more C features,
% and in tow, a richer separation logic.
\begin{itemize}
    \item \textbf{User-defined variadic functions.} I believe the types of
        \kl{CN} are rich enough to support this, but supporting the
        \kl{Cerberus} constructs which enable this will require more work.
    \item \textbf{Function pointers.} Supporting this in the general
        case would require a higher-order separation logic, which would
        be considerable departure from the current design and require
        thinking deeply about reasonable schemes and expectations for
        automation.
    \item \textbf{Standard library support.} Full support for the above
        two features should allow this straightforwardly, though
        there may be considerable engineering involved.
    \item \textbf{Locks and relaxed-memory concurrency.} A first-order solution
        would be to provide locks as a primitive, and that should suffice in
        many cases, but at the very least would require fractional permissions
        to make usable. Relaxed memory models will require an even richer
        separation logic; as before, the impact on automation is unknown.
    \item \textbf{Inline assembly.} As seen in the early allocator in
        \kl{pKVM}, inline assembly is used in real-world C, and supporting
        this soundly would be a rather large integration with frameworks
        such as Islaris~\sidecite{sammler2022islaris}.
    \item \textbf{Binary verification.} Mete Polat did some exploratory
        work~\sidecite{polat2023automated} on translating \kl{CN}
        specifications and proofs to binary using the
        Islaris framework, and turning this into full-fledged support
        for direct binary verification (as opposed to verifying an
        industrial-grade optimising compiler) would be a large undertaking.
\end{itemize}

\section{More performance}

\begin{itemize}
    \item \textbf{Optimising solvers for typical queries.} It seems
        likely that the types of queries \kl{CN} will typically send a
        solver are quite amenable to automation, especially related
        to pointers and the \kl{CN-VIP} memory model. If these can be
        made substantially faster, then this would have a large
        benefit on \kl{CN} performance.
    \item \textbf{Carefully incrementalisng verification.} For large projects,
        \kl{CN} should cache proofs it already knows and only check the parts
        which have changed.
\end{itemize}

\section{More user-friendliness}

\begin{itemize}
    \item \textbf{Iterating on syntax with user studies.} Syntax matters
        a lot to most people outside of programming language research,
        and the syntax of \kl{CN} right now is elegant but verbose. It
        needs to be made simultaneously more concise and intuitive, whilst
        retaining a clear denotation. C programmers may prefer an imperative
        specification language, and this may be more feasible than one might
        expect~\sidecite{memarian2024ghost}, accommodating which would be a new
        challenge for specification research.
    \item \textbf{Experimenting with graphical representations of resources.}
        It is difficult to understand failed resource lookups in \kl{CN}.
        On top of this, although currently \kl{CN}'s ownership model is simple,
        features such as fractional permissions will add further complexity to
        this. It would be useful to experiment with graphical explanations of
        resource manipulations, either representations of a symbolic heap, or
        resource change timelines Ã  la
        RustViz\sidenote{\url{https://fplab.github.io/rustviz-tutorial/}}~\sidecite{almeida2022rustviz}.
        Of course a key difference between C and Rust is not tying aliasing
        to syntax, and so (potential) aliases would need to be explained
        appropriately.
    \item \textbf{Pedagogy and incremental benefits for professionals.}
        The target audience of \kl{CN} is people who are more invested in
        correctness and learning than the average programmer, but will still
        have limited time in which to become productive and show benefits
        to stakeholders. Even if the preceding two ideas are executed perfectly,
        verification is different enough from programming and test-based tooling
        to require structured pedagogy. On top of this, it will always be
        competing with the other two for attention. A sensible pedagogy,
        combined with incremental benefits (such as runtime checking of
        incomplete assertions~\sidecite{banerjee2025fulminate}), will
        be required for the substantial uptake.
    \item \textbf{Suggesting specifications, proofs and repairs.} Aside from
        being difficult, verification is often tedious. There is rich history
        of using techniques from artificial intelligence to improve the
        automation of both SAT solvers and interactive proof assistants, and
        there is much excitement about the potential for using machine
        learning, particularly large language models, for improving on the
        state-of-the art~\sidecite{first2023ml4tp}.
\end{itemize}
