\bookmarksetup{startatroot}
\chapter{Conclusion}%
\label{chap:conclusion}

\margintoc{}

In this thesis, I argued that building a verification tool for C, suitable
for handling low-level systems programming idioms, is two parts engineering,
and one part theory.

In~\arefpart{formalisation}, I discussed the origin of \kl{CN} as a tool
to verify the \kl{pKVM} hypervisor, and \kl{CN}'s design goal of lowering the
cost of verification and being usable by `systems programmers who know
Haskell'. I showed how these engineering constraints informed the choice of
\kl{CN}'s many theoretical foundations. I showed that those these foundations
are well-established and individually well understood, its application to a
large calculus \kl{Core} with unique features presents new challenges and
solutions, such as closely linking resource contexts to heaps by representing
them as separation logic predicates, and using explicit proof terms to encode
transformations on both. I concluded by feeding back insights from the theory
into actual engineering questions we face in developing and extending \kl{CN}.

In~\arefpart{mem-model}, I described a complex, formal, memory object
model for C which accurately captures most of the behaviour exhibited by
existing optimsing compilers. I showed that we can simplify some of the
complexity by exploiting the fact that in verification, small and
easy-to-explain changes to the code are justified if they make the typing rules
easier to implement and explain. These engineering constraints defined a
flexible perimeter for the design space, which I decomposed, guided by the
theory of the model's definitions, features, purposes, and representations in
the \kl{CN} type system. I showed that there are multiple reasonable design
points for typing rules to capture, and that doing so soundly can come down to
very subtle technical details. After summarising the point I chose for
\kl{CN-VIP}, I showed that factoring out the definition and use heap as I did
earlier allows for a significantly more modular updates and structure to the
overall proof of soundness. I then explained how I implemented these rules in
\kl{CN} and their effect on the annotation and performance overhead of \kl{CN}.
I concluded by feeding back insights from the theory to improve performance,
and sketch out the development of new features such as support for lemmas and
better foundations for \kl{CN}.

Lastly, in~\arefpart{engineering}, I showed that a verification tool for
real-world C, needs some way of taming the complexity of large repositories and
non-standard or unsupported aspects of C. I showed that whilst \kl{CN}'s first
sub-goal of proving \kl{pKVM} seems to be feasible, there are several concrete
ways in which needs to become more usable. I showed that the second sub-goal of
on-going proof maintenance still needs substantial work. I then compared
\kl{CN} to other proof tools by discussing my experience on verifying a small
but informative example in all of them, which indicated where it stands with
respect to the academic state-of-the-art. I then discussed industry feedback on
\kl{CN}, which indicated of where \kl{CN} stands in relation to user
expectations. Lastly, I synthesised the large gap between them by outlining
concrete expectations and how to achieve them for \kl{CN} and similar tools.

There are of course, longer-term lines of research to pursue based on \kl{CN}.
I see at least four broad directions for this. I already covered in detail the
direction of more trustworthiness in \nameref{sec:better-foundations}; I will
discuss the remaining aspects here.

\section{More expressiveness}

% Increasing expressiveness of \kl{CN} means adding support for more C features,
% and in tow, a richer separation logic.
\begin{itemize}
    \item \textbf{Support for unions.} This is a type system design
        question, and is expected to be straightforward.
    \item \textbf{User-defined variadic functions.} I believe the types of
        \kl{CN} are rich enough to support this, but supporting the
        \kl{Cerberus} constructs which enable this will require more work.
    \item \textbf{Function pointers.} Supporting this in the general
        case would require a higher-order separation logic, which would
        be considerable departure from the current design and require
        thinking deeply about reasonable schemes and expectations for
        automation.
    \item \textbf{Standard library support.} Full support for the above 
        two features should allow this straightforwardly, though
        there may be considerable engineering involved.
    \item \textbf{Locks and relaxed-memory concurrency.} A first-order solution
        would be to provide locks as a primitive, and that should suffice in
        many cases, but at the very least would require fractional permissions
        to make usable. Relaxed memory models will require an even richer
        separation logic; as before, the impact on automation is unknown.
    \item \textbf{Inline assembly.} As seen in the early allocator in
        \kl{pKVM}, inline assembly is used in real-world C, and supporting
        this soundly would be a rather large integration with frameworks
        such as Islaris.~\sidecite{sammler2022islaris}
    \item \textbf{Binary verification.} Mete Polat did some exploratory
        work~\sidecite{polat2023automated} on translating \kl{CN}
        specifications and proofs to binary using the
        Islaris framework, and turning this into full-fledge support
        for direct binary verification (as opposed to verifying an
        industrial-grade optimising compiler) would be a large undertaking.
\end{itemize}

\section{More performance}

\begin{itemize}
    \item \textbf{Inferring join-points for resource contexts.} If this
        was possible, even many if not all cases, then this would
        reduce the chances of state-explosion affecting verification
        complexity.
    \item \textbf{Optimising solvers for typical queries.} It seems
        likely that that the types of queries \kl{CN} will typically send a
        solver are quite amenable to automation, especially related
        to pointers and the \kl{CN-VIP} memory model. If these can be
        made substantially faster, then this would have a large
        benefit on \kl{CN} performance.
    \item \textbf{Carefully incrementalisng verification.} For large projects,
        \kl{CN} should cache proofs it already knows and only check the parts
        which have changed.
\end{itemize}

\section{More user-friendliness}

\begin{itemize}
    \item \textbf{Iterating on syntax with user studies.} Syntax matters
        a lot to most people outside of programming language research,
        and the syntax of \kl{CN} right now is elegant but verbose. It
        needs to be made simultaneously more concise and intuitive, whilst
        retaining a clear denotation. C programmers may prefer an imperative
        specification language, and this may be more feasible than one might
        expect~\sidecite{memarian2024ghost}, accommodating which would be a new
        challenge for specification research.
    \item \textbf{Experimenting with graphical representations of resources.}
        It is difficult to understanding failed resource lookups in \kl{CN}.
        On top of this, although currently \kl{CN}'s ownership model is simple,
        features such as fractional permissions will add further complexity
        this. It would be useful to experiment with graphical explanations of
        resource manipulations, either representations of a symbolic heap, or
        resource change timelines Ã  la
        RustViz\sidenote{\url{https://fplab.github.io/rustviz-tutorial/}}~\sidecite{almeida2022rustviz}.
        Of course a key difference between C and Rust is not tying aliasing
        to syntax, and so (potential) aliases would need to be explained
        appropriately.
    \item \textbf{Pedagogy and incremental benefits for professionals.}
        The target audience of \kl{CN} is people who are more invested in
        correctness and learning than the average programmer, but will still
        have limited time in which to become productive and show benefits
        to stakeholders. Even if the preceding two ideas are executed perfectly,
        verification is different enough to programming and test-based tooling
        to require structured pedagogy. On top of this, it will always be
        competing with the other two for attention. A sensible pedagogy,
        combined with incremental benefits (such as runtime checking of
        incomplete assertions~\sidecite{banerjee2025fulminate}), will
        be required for the substantial uptake.
    \item \textbf{Suggesting specifications, proofs and repairs.} Aside from
        being difficult, verification is often tedious. There is rich history
        of using techniques from artificial intelligence to improve the
        automation of both SAT solvers and interactive proof assistants, and
        there is much excitement about the potential for using machine
        learning, particularly large language models, for improving on the
        state-of-the art.~\sidecite{first2023ml4tp}
\end{itemize}

