In \cref{sec:c-lang}, I discussed how competing forces of inherited
portability requirements, proximity to hardware, and the desire for more
aggressive optimisations led to complex and subtle technical resolution by
stakeholders in the \kl{ISO} standard of C. I also mentioned that its nature as
a prose document, with natural language ambiguities and omissions, as well as
divergence from C as used \kl{de facto}, mean that its semantics are unreasonable
for a human to adhere to, and challenging to build into tools directly,
without making some sort of simplifying assumptions.

Existing program logic frameworks for C such as Verifiable C~\sidecite{appelSF5}
and RefinedC~\sidecite{sammler2021refinedc} take the approach of building a
logic directly above an operational semantics for a language which is
recognisably C, minus some desugaring to consolidate similar constructs. They
attempt to retain as many C features (control flow, variable scoping, aliasing,
loose evaluation order, pointer manipulation rules) as possible, but make
simplifying assumptions where it would be impractical otherwise.

Given that \kl{CN}'s headline goal (\cref{sec:cn-intro}) is to work with
pre-existing C programs, which rely on many if not all of those impractical
features, adopting the conventional approach would quickly use up most of its
complexity budget and make the other goal (of reducing the expertise required
to do verification) unfeasible.

Instead, \kl{CN} builds directly upon the
\kl{Cerberus}~\sidecite{memarian2022cerberus} executable and empirically
validated semantics for C. Not only does \kl{CN} benefit from the
\emph{accurate} semantics for both \kl{ISO} and \kl{de facto} C, it benefits
most from the \emph{usability} of it. This is because, Cerberus is elaborated
into a relatively small calculus \emph{\kl{Core}}, which translates all of C's
complexity into a first-order functional language with a few special (but easy
to understand and specify) constructs.

Additionally, \kl{CN} is intended to be used more like a \emph{type system} in
an IDE than a program logic inside a proof assistant. Ideally, instead of
seeing intermediate goals in a sophisticated separation logic, and needing to
be well versed with a range of inference rules and automation tactics, a user
sees their C program, scattered with predictable and lightweight annotations in
comments, in an editor which either indicates success, or clear and helpful
error message.

Aside from the fact that the notion and mode of use of a type system is more
familiar to most programmers (an advantage not to be scoffed at), this approach
also allows \kl{CN} to use and advance the extant literature on building
refinement type systems on top of existing languages.

This type system approach also leads to other desiderata and their
corresponding responses. If we want to follow a type system approach, we want
to minimise obvious annotations and justify why the necessary ones are so, we
need to track carefully the flow of information in the type system, using a
\kl{bidirectional} approach. We also need some sort of automation so as to not
burden the programmer with proving things like $1 + 1 = 2$. Similar to
VeriFast~\sidecite{jacobs2011verifast} and Frama-C~\sidecite{kirchner2015frama},
\kl{CN} enlists the support of SMT solvers to mitigate this. When trying to
verify code against expressive specifications, this could lead to
non-termination, so \kl{CN} also restricts the expressiveness of the assertion
language, and the queries it sends to the SMT solver. And given the importance
of managing resources in C, the typing discipline needs to be substructural.

The \kl{CN} assertion language syntax aims to be expressive enough to verify
real world C, but also restricted enough to limit the aforementioned technical
problems, and intuitive enough to a target audience of systems programmers who
happen to know Haskell (or Rust).

With this many constraints and design decisions, it is easy to be doubtful of the
elegance and feasibility of this approach, let alone consider proving such a
type system sound. As I will show in \nameref{chap:kernel-statics}, whilst the setup
might be novel, multi-faceted and large, the definitions are relatively
straightforward, and the proof of soundness can be done syntactically. Both the
definitions and the proof are modular with respect to the heap, so that
changing the memory object model does not require redoing the entire soundness
proof. The formalisation is close enough to the surface syntax of \kl{CN} so
that a correspondence between the two can be stated simply and precisely, and
close enough to the implementation to offer actionable insights.

\chapter{Formalisation Background}%
\label{chap:formal-background}

\margintoc{}

The components of \intro{Kernel CN} all have precedent in prior work; the main
new contribution is the adaptation and confluence of those ideas. This chapter
will set out \kl{CN}'s design goals and origins, recapitulate the disparate
concepts used in CN, and along the way discuss how they satisfy the
aforementioned design goals.

\section{\kl{CN} design goals and constraints}%
\label{sec:cn-goals}

Aiming for \emph{``a verification tool whose aspirational goal is to lower the
cost of C verification from a Rocq programmer who knows separation logic to a
systems programmer who knows Haskell''} (\cref{sec:cn-intro}) helps narrow
down the large design space of verification tools.

The reason for picking this particular goal is in \kl{CN}'s origins as
an attempt to verify the pKVM hypervisor, developed by Google.

To explain pKVM, I need to situate it in its context: the Android
operating system, which runs on billions of devices worldwide, plays a central role
in many lives, including handling an enormous amount of sensitive data. This
means that security is paramount, however because each device runs its own
kernel (up to half the code is not Android's version of Linux), updates are
very challenging and expensive to test and deploy to each device. Aside from
security issues, this also leads to fragmentation of Android, so devices and
apps are not all up-to-date and the long delay (at least 18 months) between
Linux and device releases makes it difficult to upstream features and fixes.%
\sidenote{%
TODO\@: cite these properly.
\begin{itemize}
    \item \url{https://youtu.be/7novnkldMmQ?feature=shared}
    \item \url{https://youtu.be/wY-u6n75iXc?feature=shared} and \url{https://lwn.net/Articles/836693/}
    \item \url{https://source.android.com/docs/core/architecture/kernel/generic-kernel-image}
    \item \url{https://source.android.com/docs/core/virtualization/whyavf}
    \item \url{https://googleprojectzero.blogspot.com/2020/02/mitigations-are-attack-surface-too.html}
    \item \url{https://lpc.events/event/7/contributions/780/}
\end{itemize}
}

Whilst some of this has been mitigated with the introduction of
\intro[GKI]{Generic Kernel Images (GKI)}, which provide a small and stable
kernel ABI \emph{for a particular long-term release} version of Android, there
are still security issues present in this model, because the kernel is too
large (20 million lines of code) to be a reasonable trusted computing base, and
the drivers vendors ship with a device are part of it.

Some manufacturers use hypervisors, which attempt to isolate the kernel from
the rest of the system by running Android and other hardware components in
virtual machines, such `secure' parts of the device storing sensitive data.
Aside from security, hypervisors are also used to partition memory at boot-time
so that devices can use it for things like direct memory access, and run
arbitrary code outside of Android, which is worrying because this code would
run at a more privileged level than Android itself. All of this just
\emph{shifts} the attack surface, and has also resulted in \emph{more}
fragmentation at the hypervisor layer.

Similar to GKI, the proposed solution to standardise the hypervisor used. There
is already a mature hypervisor which is part of the Linux kernel, the
Kernel-based Virtual Machine (KVM)\@. It is set up so that a host kernel can
dynamically allocate virtual machines for guests to run on, and protect the
host from the guests. However, at the start of the project, the API exposed by
the hypervisor to the host kernel offered too much control, and guests were not
protected from the \emph{host}. This is a problem because the guest could be
running code for a secure hardware component (e.g.\ a biometric authenticator),
the host could be a compromised version of Android, so an attacker could still
get access to sensitive information.

To solve this, Google, as part of the Android Virtualisation Framework, is
developing a \intro[pKVM]{protected KVM (pKVM)}, which runs \emph{underneath}
the kernel, and ships \emph{as part} of the kernel image. Not only does this
tight coupling remove issues around ABI compatibility between the hypervisor
and the kernel, since the source is always in the same repository, it also
allows pKVM to commit to only handling implementing a select few functions such
as virtual memory management and remain very small, and rely on the Linux
kernel to manage the rest, such as scheduling, device drivers and power
management.

If successful, this could make the attack surface a lot smaller, but it could
also make it one that is used very widely. It is in this context that Google
sought assistance from the research community to see if verifying the kernel
was feasible, \emph{on an ongoing basis}. A one-and-done verification of pKVM
which takes an army of PhD students and postdocs a few years to verify and is
years out of date by the time is developed is not worth the investment; a
tool which C kernel programmers can understand, use and maintain proofs as they
make changes to very important security critical code is.

So not only does this background explain \kl{CN}'s headline goal, it also
clarifies some of the \emph{constraints} on its design:
\begin{itemize}
    \item Because kernel programmers wrote the code, and are intending to use
        conventional compilers to build and run it,\sidenote{Assuming the
        binary can be verified as well, perhaps with input from \kl{CN}.} we
        cannot rely on (hopefully sound) approximation to the semantics of C
        \textemdash{} we want and need something that matches and can handle
        its real world behaviour as closely as possible.
    \item Because it will be used by kernel programmers, we want a story that is
        is accessible and acceptable to them. These are very smart and
        capable people, who do not have the time or support to get up to
        speed with separation logics and interactive proof assistants, or be
        amenable to change their (or more importantly, their organisations)
        workflows substantially. A ``fancier type system'' which runs as part of
        the \intro[CI]{continuous integration (CI)} pipeline is much more
        likely to be used and adopted in this context.
    \item Similarly, because the annotations will be read by kernel programmers,
        and upstreamed into Linux, we want them to be minimal and relatively
        easy to understand. Not only does this affect the design of the type
        system to manage the flow of information carefully, this encourages
        exploring how best to automate as many obvious things as feasible.
\end{itemize}

In turn, these constraints feed into concrete technical choices which \kl{CN}
makes:
\begin{itemize}
    \item To capture real-word C behaviour, \kl{CN} uses the \kl{Cerberus}
        empirically validated semantics.
    \item To integrate into existing workflows, \kl{CN} appears to users
        as a fancy type system.
    \item To retrofit the type system on top of existing ones, and in
	particular to ensure erasure, \kl{CN}'s type system uses
	\intro{refinement types}.
    \item To automate proofs of constraints, \kl{CN} relies on SMT solvers.
    \item To ensure decidability (termination), and aim for reasonable in
        practice performance, \kl{CN} follows a \emph{liquid} typing
	discipline, restricting the queries it sends to the SMT solver.
    \item To minimise constraint annotations, \kl{CN}'s type system is
        formalised and implemented in a \kl{bidirectional} style.
    \item To check the resource management of C programs, \kl{CN}'s type
        system uses \intro{linear} types, using the grammar of separation
        logic assertions.
    \item To stay within the \kl{liquid} typing discipline \emph{and} avoid
	quantifier instantiation annotations, \kl{CN} uses precise assertions
	with a \kl{mode}d discipline for predicate arguments.
    \item To make such precise assertions with a mode discipline easier to read,
	write and explain, \kl{CN} presents users with \emph{monadic} syntax.
\end{itemize}

\section{Cerberus and Core for a usable and accurate C semantics}%
\label{sec:cerberus-core}

\intro{Cerberus} is an empirically validated and executable semantics for
\kl{ISO} and \kl{de facto} C, specifically C11. A detailed comparison
between it and other C semantics is available in the Related Work chapter
of \sidetextcite{memarian2022cerberus}, but for the purposes of \kl{CN},
it suffices to say it captures real-world C.

Where \kl{Cerberus} really shines with respect to \kl{CN}'s use case in
\emph{how} it captures this executable semantics. In particular, it does so by
\emph{compositional elaboration} into a \emph{relatively simple first-order
functional language}, unlike other semantics, which are defined over some
desugared and consolidated grammar closely resembling C.

This approach drastically simplifies many tricky parts of C. For example,
\cref{fig:perplexing-ub} is accepted by the frontend, complete with strange
scoping and strange control flow which jumps \emph{into} a loop body. This
program is illegal because of the subtle rules regarding block scopes, object
lifetimes and initialisation (as explained in the caption). Whilst contrived,
any C function which happens to have exception handling and retry behaviour,
and be longer than a programmer's screen height, could make the same error.
Cerberus handles C features such as mutable and potentially aliasing variables,
and the myriad other \kl[UB]{undefined} or \kl{unspecified} behaviours, for
example loose evaluation order and coercions respectively, by making them all
\emph{explicit} in the syntax.

\begin{marginfigure}
    \cfile[breaklines]{code/perplexing-ub.c}
    \caption{Example of subtle scoping and control-flow issues leading to UB
        (courtesy~\cite[p70]{memarian2022cerberus}).
        Jumping to \cinline{l2} starts the lifetime of an object, but does not
        initialise it. Jumping to \cinline{l1} then \emph{ends that lifetime}
        because it exits the block (lines 5\textendash{}12). By the time
        execution reaches line 8, \cinline{p} \emph{refers to a dead object}.
        This is why the seemingly redundant re-assignment of \cinline{p = &x}
        on line 7 would prevent UB in this example.}\label{fig:perplexing-ub}
\end{marginfigure}

\section{Core grammar}\label{sec:core-grammar}

Instead of working directly over something similar to C and trying to express
static and dynamic semantics over something so complex, \kl{Cerberus}
elaborates C into \intro{Core}, a not-too-large calculus where each construct
is designed to capture some peculiarity of C. It is split into two fragments, a
pure (\cref{fig:pure-core-grammar}) and effectful
(\cref{fig:effectful-core-grammar}) which embeds the pure one.

The pure fragment is pure in the sense that it allows no memory operations, but does
include the effect of undefined behaviour explicitly with the
\coreinline{undef()} operator. This aspect of the language handles % chktex 36
things like implicit type conversions or bounded arithmetic. As visible from the
grammar, the pure part is very much a first-order functional language with
recursive functions with some constructs specific to C such as pointer
arithmetic on arrays and struct fields, structs, unions and
specified/unspecified/integer values.

\begin{figure*}[tp]
    \ContinuedFloat*
    \raggedright%
    \small%
    \cngrammarcompressed{\textwidth}{%
        \cnpce{}\cninterrule{}
        \cnname{}\cninterrule{}
        \cncpat{}\cninterrule{}
        \cnctor{}\cnafterlastrule{}
    }
    \caption{The pure fragment of Core.}\label{fig:pure-core-grammar}
\end{figure*}

The effectful fragment captures interactions with memory (via a memory
interface), various ordering constraints, and more exotic control flow with a
goto-like operator used in the elaboration of C's iteration and \cinline{goto}
statements. The distinction between the pure and effectful fragments is in
fact unrelated to the distinction between expressions and statements in C,
since both are elaborated into effectful expressions (for example,
\coreinline{PtrEq} which tests for for pointer equality).

I will discuss \coreinline{memop()} in more detail in % chktex 36
\nameref{chap:mem-model-explained}. For now it suffices to say that the
operations are effects, part of the memory interface \kl{Core} uses to abstract
over choices of different handlers, implementations of those effects in a
specific memory object model.

The following constructs are all related to evaluation order:
\coreinline{neg()}, \coreinline{unseq()}, \coreinline{let weak}, % chktex 36
\coreinline{let strong}, \coreinline{bound()}. These were supported % chktex 36
in the implementation at one point,\sidenote{See note~\ref{sn:new-inf}.} but
stopped working due to a change in the resource inference scheme, and not
enough of a priority to re-enable. I did not attempt to formalise their complex
operational behaviour; I considered supporting these constructs using
fractional permissions, but decided to leave that for future work because of
the noise and complexity it would add to the initial version of the
formalisation. The \coreinline{par()} construct is for spawning % chktex 36
threads. The \coreinline{nd()} construct models non-determinism % chktex 36
as required or implied by aspects of the standard.

The \coreinline{ccall()} and \coreinline{pcall()} constructs for % chktex 36
calling elaborate C functions and Core procedures (effectful functions)
respectively. They differ only in how the name of the procedure to be called is
found, with \coreinline{ccall()} using the memory interface to do so. % chktex 36

The (thread-local) Core operational semantics is defined as a small-step
transition from a configuration to either undefined behaviour or a new
configuration. Formally, it looks like this $\langle h, E, \kappa \rangle
\rightarrow (\textsc{Undef} \mid{} \langle h', E', \kappa' \rangle)$, for a
heap $h$, effectful expressions $E$ and stack $\kappa$ (list of evaluation
contexts $C$). These steps rely on evaluating pure expressions (in a big-step
style) to either undefined behaviour or a value $\cnnt{pce} \Downarrow
(\textsc{Undef} \mid{} \cnnt{value})$.

For example, procedure calls pop the enclosing evaluation context $C$ on
to the stack $\kappa$, evaluate the arguments into values, lookup the function
body $E$ and substitute the parameters for the values, and proceed with
executing the body.

{\small%
\[
\inferrule[{[PCall]}]
 { \mathrm{funmap} ( \cnnt{name} ) = \left(\cncomp{x_i {:} \beta_i}{i}\right).\ E \\
   \cncomp{\cnnt{pce}_i \Downarrow \cnnt{value}_i}{i} }
 { \left< h , C\left[ \cnkw{pcall} \, \left( \cnnt{name}, \cncomp{\cnnt{pce}_i}{i} \right) \right] , \kappa \right>
   \rightarrow \left< h , \left[ \cncomp{\cnnt{value}_i / x_i}{i} \right] E , C \Colon \kappa \right> }
\]}

Conversely, returns pop an evaluation context off of the stack, applies it to
the values, and proceeds with the resulting expression.

{\small%
\[
\inferrule[{[Return]}]
  { }
  { \langle \cnnt{h} , \cnkw{pure}(v) , C \Colon \kappa \rangle
    \rightarrow \langle h, C [\cnkw{pure}(v)] , \kappa \rangle }
\]}

The \coreinline{save()} operator is \kl{Core}'s way of introducing named % chktex 36
continuations with default arguments. The label $l$ and arguments $x_1, \ldots,
x_n$ are in scope in $E$; those variables are associated pure expressions
provided by the \coreinline{run()} operator or with the default $e_1, \ldots, % chktex 36
e_n$, if the operator is reached otherwise. This is the \cinline{goto}-like
operator referred to earlier, which is used to elaborate all of C's iteration
and \cinline{goto} statements.

Formally, reaching a label via \coreinline{save()} evaluates its % chktex 36
default expressions into values and then substitutes those values into its
expression.\sidenote{Since this does not manipulate the heap or the stack, it
    is phrased purely as is expression reduction, with a separated, omitted
    rule to lift expression reductions to thread-local reductions.}

{\small%
\[
\inferrule[{[Save]}]
  { \cncomp{\cnnt{e}_i \Downarrow{} \cnnt{value}_i}{i} }
  { \cnkw{save} \, \cnnt{id} \left( \cncomp{x_i \cnkw{:=} e_i}{i} \right) \, \cnkw{in} \, \cnnt{E}
    \rightsquigarrow \left[ \cncomp{ \cnnt{value}_i / x_i }{i} \right] E }
\]}

Reaching the same label via a \coreinline{run()} however, is more % chktex 36
complicated. Like a procedure call, it evaluates its arguments into values and
looks up the label $\cnnt{id}$. Unlike a procedure call, the result of the
lookup is \emph{a context} $C_{\cnnt{id}}$ and an expression $E_{\cnnt{id}}$.
It substitutes the values for the parameters in $E_\cnnt{id}$, \emph{discards
the current context} $C$, and uses $C_\cnnt{id}$ instead. All the while, the
stack $\kappa$ is unchanged.

{\small%
\[
\inferrule[{[Run]}]
  { \mathrm{labelmap} ( \cnnt{id} ) = \left(\cncomp{x_i}{i}\right).\ C_{\cnnt{id}} [ E_{\cnnt{id}} ] \\
    \cncomp{e_i \Downarrow \cnnt{value}_i}{i} }
  { \langle h , C\left[ \cnkw{run}\, \cnnt{id} \left( \cncomp{e_i}{i} \right) \right] , \kappa \rangle
    \rightarrow \left< h , C_{\cnnt{id}} \left[ \left[ \cncomp{\cnnt{value}_i / x_i}{i} \right] E_{\cnnt{id}} \right] , \kappa \right> }
\]}

The important aspect for our purposes is that while control flows into
\coreinline{ccall()}, \coreinline{pcall()} and \coreinline{run()}, it % chktex 36
only returns from the first two and not the last.

\begin{figure*}[tp]
    \ContinuedFloat{}
    \raggedright%
    \small%
    \cngrammarcompressed{\textwidth}{%
        \cncore{}\cninterrule{}
        \cncoreXXctxt{}\cninterrule{}
        \cncmemop{}\cninterrule{}
        \cncaction{}\cnafterlastrule{}
    }
    \caption{The effectful fragment of Core.}\label{fig:effectful-core-grammar}
\end{figure*}

\section{Elaboration example: list append}

We can see an example of Cerberus' elaboration into \kl{Core}, by recalling the
linked integer list append function from \cref{fig:append-c}.

The first thing it does it declare its parameters. In C, these are mutable
local variables.

\cfile[firstline=5, lastline=6]{code/append_plain.c}

In Core, the pointer value passed into the function, and the mutable storage
for it are separated syntactically. The parameters have type
\coreinline{loaded pointer}, where \coreinline{loaded t} is either a
\coreinline{Specified(v:t)} or an \coreinline{Unspecified} value % chktex 36
(an option monad for potentially uninitialised values read from memory). The
result of the function has type \coreinline{eff loaded t} where
\coreinline{eff t} is a value of type \coreinline{t} produced with effects
(also a monad). The \coreinline{create()} and \coreinline{store()} operations are % chktex 36
memory actions, and produce values of type \coreinline{eff pointer} and
\coreinline{eff unit} respectively. Binding constructs \coreinline{let strong}
and \coreinline{let weak} (monadically) bind effectful values, and sequence
their effects.\sidenote{$\cnkw{let}\ \cnkw{strong}\ \cnnt{pat}\ \cnkw{=}\
    \cnnt{E}_1\ \cnkw{in}\ \cnnt{E}_2$ sequences all memory
    actions performed by $\cnnt{E}_1$ before those performed by $\cnnt{E}_2$.
    $\cnkw{let}\ \cnkw{weak}$ only sequences positive memory actions similarly;
    in other words, for $\cnkw{let}\ \cnkw{weak}\ \cnnt{pat}\ \cnkw{=}\
    \cnnt{E}_1\ \cnkw{in}\ \cnnt{E}_2$, any negative memory actions
    ($\cnkw{neg}(\cnnt{action})$) performed by $\cnnt{E}_1$ are
    unsequenced with respect to all memory actions performed by $\cnnt{E}_2$.
    A race only occurs if unsequenced memory actions have overlapping
   footprints, and is deemed \kl{UB}.}
In this example, weak sequencing has no observable effect, and so is
safe to read as \coreinline{let strong}.

\corefile[firstline=3, lastline=10]{code/append_plain.core}


In C, the test to check if \cinline{x} is \cinline{NULL} looks like this.

\cfile[firstline=8, lastline=8]{code/append_plain.c}

First this requires loading the \coreinline{xs} from memory, and then doing a
\coreinline{PtrEq} memory operation to check if it is \coreinline{NULL}.
Because booleans are represented by 0 and 1 in C, the result of this expression
is a \coreinline{loaded integer}.

\corefile[firstline=15, lastline=25]{code/append_plain.core}

That result is bound to \coreinline{a_432} and then converted (omitted) into a
Core boolean, bound to \coreinline{a_431}, which is in turn used in a Core
if-expression. Recall the true-branch of the C code.

\cfile[firstline=9, lastline=9]{code/append_plain.c}

This elaborates into the following true-branch in Core. First, the value
\coreinline{ys} is loaded from memory. Core marks the boundary of a \emph{full
expression} (ยง6.8p4), such as the optional expression of a return statement,
using the \coreinline{bound()} operator. After that is one % chktex 36
\coreinline{kill()} per live local variable. Lastly there is a % chktex 36
\coreinline{run()} to the return label \coreinline{ret_430}. % chktex 36

\corefile[firstline=48, lastline=56]{code/append_plain.core}

The \coreinline{ret_430} label is defined with a \coreinline{save}, at the end
of the procedure. Like C labels, Core labels can be reached by fall-through
execution as well as being jumped to. In the fall-through case, the arguments
to the label still need to be given a value; these default values are defined
by the right-hand-side of the \coreinline{:=}, here an \coreinline{undef()} % chktex 36
value because it is UB to reach the end of a function with a non-void return
type without returning a value.

\corefile[firstline=143, lastline=144]{code/append_plain.core}

In the C code, the else-branch defines a new local variable \cinline{new_tail},
and assigns it to the result of a recursive call.

\cfile[firstline=11, lastline=11]{code/append_plain.c}

In Core, these two lines are expanded considerably, so I shall omit some
details and focus only on the key parts. First is the allocation of memory for
the local variable.

\corefile[firstline=59, lastline=60]{code/append_plain.core}

Next is getting the pointer for the C function to be called. This expression is
bound to \coreinline{a_447}.

\corefile[firstline=59, lastline=60]{code/append_plain.core}

The first argument is \cinline{xs->tail}; in Core that is represented by a
\coreinline{load()} of \cinline{&xs}, a case-split on the loaded value % chktex 36
(UB if it is \coreinline{Unspecified}), a \coreinline{member_shift()}, % chktex 36
and then another \coreinline{load()}. The whole expression is bound % chktex 36
to \coreinline{a_454}.

\corefile[firstline=71, lastline=81]{code/append_plain.core}

The second argument is \cinline{ys}; in Core that is just a \coreinline{load()} % chktex 36
of \cinline{&ys}. The whole expression is bound to \coreinline{a_461}.

\corefile[firstline=83, lastline=84]{code/append_plain.core}

Finally, the elaboration of the function call itself performs a large amount of
run-time type checking; in the general case this supports calling variadic
functions via function pointers, but for this example, because the function is
(a) known and compile time and (b) not variadic, it simplifies to the below.

\begin{corecode}
ccall("struct int_list* (*) (struct int_list*, struct int_list*)",
      a_447 (* &IntList_append *),
      a_454 (* xs->tail *),
      a_461 (* ys *))
\end{corecode}

The result of this is in turn bound to \coreinline{a_445}, which is then
stored in at \cinline{&new_tail} (in Core, just \coreinline{new_tail}).

\corefile[firstline=105, lastline=107]{code/append_plain.core}

Penultimately, the \cinline{IntList_append} function needs to update the tail
of the first list.

\cfile[firstline=12, lastline=12]{code/append_plain.c}

In Core, \cinline{xs->tail} is elaborated into a \coreinline{load()} % chktex 36
and a member shift, whose result is bound to \coreinline{a_472}.

\corefile[firstline=110, lastline=118]{code/append_plain.core}

The right-hand-side is just a \coreinline{load()} from \cinline{&new_tail} % chktex 36
(which in Core, is just \coreinline{new_tail}), whose result is bound to
\coreinline{a_479}.

\corefile[firstline=122, lastline=123]{code/append_plain.core}

In C, assignments can be used as expressions. In the jargon of the standard,
any loads used to evaluate the operands form a \emph{value computation}, but
the stores do not: they are classed as \emph{side effects}.\sidenote{%
ยง5.1.2.3\#2: [\ldots] Evaluation of an expression in general includes both
value computations and initiation of side effects.} As
per~\textcite[p61, p66, p99]{memarian2022cerberus}, \emph{``intuitively, a Core action
is negative when it elaborates what the C standard calls a `side
effect'\,''}.
% p61 and p66 have more about value computations and side effects

\corefile[firstline=125, lastline=129]{code/append_plain.core}

Finally, there's the return statement at the end of the function.

\cfile[firstline=13, lastline=13]{code/append_plain.c}

In Core, this is elaborated into the following. First, a
\coreinline{load()}, enclosed in a \coreinline{bound()} operator % chktex 36
to mark the boundary of the \emph{full expression} (ยง6.8p4) inside the return
statement. After that is one \coreinline{kill()} per live  % chktex 36
local variable. Lastly there is a \coreinline{run()} to the % chktex 36
return label \coreinline{ret_430} mentioned earlier.

\corefile[firstline=132, lastline=140]{code/append_plain.core}

\section{Discussion}

A few things are note-worthy about the Core elaboration:
\begin{itemize}
    \item The translation is \intro{compositional}. Each function, block,
        statement and expression is elaborated in isolation, based only
        on its parts, and follows the structure of the original C program.
    \item Each variable (function argument and local and global) is uniformly
        handled as living on the heap and gets its own storage via the
        \coreinline{create} function. Reads, writes, and de-allocations are
        represented with \coreinline{load}, \coreinline{store},
        \coreinline{kill} respectively.
    \item Loose evaluation order (for example between the expressions of a \cinline{==})
        is represented using \coreinline{unseq} and \coreinline{let weak} constructs.
    \item \kl{UB} is made explicit in the syntax of the program.
\end{itemize}

In particular, \kl{compositional}ity is just as important, if not more, than the
target language being a first-order functional language with effects. Given
that we want users to annotate programs at the C level, if we wish to type
check \kl{Core}, we need to be able to transport those annotations through the
elaboration process too, and place them at the appropriate program points.

If \kl{Cerberus} were to have elaborated into a dataflow graph instead of \kl{Core},
such transporting would be a major undertaking in itself. It may be achievable
for function pre- and postconditions, but would become much more challenging
for loops, \coreinline{goto}, and inter-statement proof
hints to \kl{CN}. With a \kl{compositional} mapping, placing annotations
structurally, and relating annotations mentioning C variables to \kl{Core}
variables becomes feasible.

In principle, the compositional mapping also ensures that errors in \kl{Core}
elaboration can be related back to useful source locations in the original C
program. However, in practice, though \emph{compositional}ity does
\emph{enable} this, it requires a good amount of engineering effort to
accomplish (\cref{sec:error-msgs}). Another challenge is that though
elaboration simplifies greatly the checked language, it also increases the
distance between the checked and the typed language, which is felt acutely when
attempting to relate failures in SMT queries back to what users wrote,
particularly when those SMT are part of an inference procedure, rather
than checking C source assertions (\cref{sec:counter-ex}).

\section{Decidable refinements for retrofitting and counter-examples}

\kl{Core} already has a straightforward \kl{bidirectional} type system as a
sanity check on the output of elaboration. It has not been used for a
rudimentary proof of soundness with respect to the dynamic semantics, since
such a theorem would be just another sanity check (on the dynamic semantics).

\begin{marginfigure}
    \centering
    \begin{mathpar}
        \inferrule{ }{\Gamma{} \vdash{} \cnkw{undef} (\cnnt{UB\_name})\Leftarrow{} \cnnt{T}}
    \end{mathpar}
    \caption{Checking rule for the \coreinline{undef()} pure expression as % chktex 36
        mentioned in \textcite{memarian2022cerberus}.}\label{fig:core-ub-typing}
\end{marginfigure}

This is evidenced by the typing rule for \coreinline{undef()} % chktex 36
(\cref{fig:core-ub-typing}). Since \kl{UB} is by definition, assumed to be
absent in a valid C program, the type system must accept any type as validly
checking against \coreinline{undef()}. % chktex 36

At a minimum, to prove C programs safe, what we would like is a type system
expressive enough to \emph{prove} the absence of \kl{UB}. If the type checking
context is enriched to track the control flow path to reach this point in the
program, for example by tracking conditions (or their negations), then the
typing rule for \coreinline{undef()} would simply require that the context % chktex 36
be inconsistent, i.e.\ able to prove false. To illustrate this point with
a trivial example, consider the nested \mintinline{py}{if} statements in
\cref{fig:dead-code}.

\begin{marginfigure}
    \inputminted[breaklines,mathescape,fontsize=\small]{py}{code/dead_code.py}
    \caption{A contrived example on how tracking control flow assumptions
        within a program could be used to prove the impossibility of
        undesirable behaviour.}\label{fig:dead-code}
\end{marginfigure}

Since we know from the outer \mintinline{py}{if} that \mintinline{py}{x} is
greater than or equal to 5, the check for \mintinline{py}{x} being strictly
less than 4 is always going to evaluate to \mintinline{py}{False} and so go to
the \mintinline{py}{else} branch rather than raise the exception. When tracking
constraints, this unreachability or dead code is represented by an inconsistent
context, from which we can prove $\mathsf{false}$.

In general, to express and track these constraints in types, we need to be able
to talk about computational variables and values \emph{at the type level},
introducing a form of dependency. However, because we do not want to change the
programming language whose type system we are enriching (such as in a
\kl{dependent type} system) we are not adding the ability for computation to
depend on types, so we retain the property of recovering the original program
and dynamic semantics after erasing the expressive types we fit on top of them.

This approach of enriching the expressiveness of types on an existing language
whilst retaining the erasure is known as \intro{refinement types}. There is a
rich history of the origins, naming and development of these
ideas,~\sidecite{jhala2021refinement} and so I will focus only on the strands
relevant to \kl{CN}.

If our goal is to minimise annotations and not require proofs for obvious
statements, then we need some sort of automation to dispatch proof
obligations.\sidenote{Recall the examples shown in
\cref{fig:call-incr,fig:call-incr-fail}, where \kl{CN} deduces
\cinline[breaklines]{y <= MAXi32()} from the constraint \cinline[breaklines]{y
<= 100i32}.} However, once we have rich constraints being tracked by the type
system, we have a first-order logic with arithmetic, proving which is
undecidable in general. There are type systems for real-world
programming languages (such as Hack or F$^*$) which are undecidable, and so
could just never terminate on some code, but have automation which works fine
in practice, so why does this point require any special consideration?

Aside from the pleasing theoretical property of having a decidable type system,
or having to tell users ``just increase the timeout'',  there are a couple of
factors which lean us in this direction, namely to be able to use SMT solvers
in a \emph{fast} and \emph{predictable} manner. Recognising that whilst
decidability can still technically mean exponential, or very often, NP-complete
complexity for problems does not negate the empirical observation that in
practice, decidable theories tend to be quite fast, and their speed (and
solvability) much less susceptible to heuristics being triggered. It is a proxy
for performance, albeit an imperfect one.

It also helps that the size of SMT problems generated by type checking
\emph{tend} to be reasonably small, at least compared to the sorts of problems
which SMT solvers are typically used for, because the amount of code humans
write between annotations is limited by cognitive capacity (and hopefully,
diligent code reviewers).

\kl{CN} ensures decidability by following restrictions on quantifiers as set
out by the liquid types~\sidecite{rondon2008liquid} approach. In practice, this
means ensuring that the constraints sent to the SMT solver are free of
quantifiers. When this approach is insufficiently expressive, \kl{CN} offers a
lemma mechanism to export and prove claims in the \kl{Rocq} interactive proof
assistant.

Aiming for decidability also has the added benefit of extracting a concrete
\kl{counter-example} (see \cref{sec:counter-ex}), which is used to explain
constraint failures in terms of concrete values for \kl{Core} program variables
(a subset of which correspond to C program variables).

\section{Bidirectionality for taming subtyping}\label{sec:bidir-subtyping}

Using a liquid typing discipline automates away the problem of tedious proof
obligations with calls to an SMT solver, but introduces the new problem of \emph{when}
during type checking is the right time to use it? This is because implications
induce a subtyping relation in the type system, and so the question becomes:
when is the correct time to use the subsumption rule, as shown in
\cref{fig:subsumption}?

\begin{marginfigure}
  \begin{mathpar}
      \inferrule{\Gamma{} \vdash{} e \mathrel{{:}} \{\, x \mid{} \phi (x) \,\}  \\ \phi (x) \Rightarrow{} \phi' (x) }
                {\Gamma{} \vdash{} e \mathrel{{:}} \{\, x \mid{} \phi' (x) \,\}}
  \end{mathpar}
  \caption{Subsumption rule using for a system with subset types and logical
      implication $\Rightarrow$ as the subtyping relation.}\label{fig:subsumption}
\end{marginfigure}

Going back to the example in \cref{fig:dead-code}, let us imagine we are in a
type system where the contexts $\Gamma$ are either $\cdot$ or $\Gamma, \{\, x \mid
\phi (x) \,\}$, where $\phi$ represents some constraint on $x$. In particular, we
are not accumulating control flow constraints in the context, only
in the variable's subset type. We would like to check that the function
\mintinline{py}{gt_zero} satisfies its postcondition. $ \{\, \mathsf{ret} \mid
\mathsf{ret} > 0 \,\} $, i.e.\ it returns a value greater than 0 and does not
raise an exception.

Working forwards, we start off with a type for $\{\, \mathsf{x} \mid \top \,\}$.
Inside the function, we are checking both branches because we cannot statically
rule one of them out. In the true case, we now \emph{refine} the type to $\{\,
\mathsf{x} \mid \mathsf{x} \geq 5 \,\}$. Because we are not sure when to apply
the subsumption rule, and we can apply it at any time, we apply it now, and
have $\{\, \mathsf{x} \mid \mathsf{x} > 0 \,\}$, because $\mathsf{x} \geq 5
\Rightarrow \mathsf{x} > 0$. At this point, we cannot statically rule out the
\cinline{else}-branch of the condition, so we start checking the one with the
exception, instead of skipping over it as dead code.

Whilst quite contrived, it illustrates the point that we would like some sort
of principled scheme to know when to use the subsumption rule, and with which
constraints. We could avoid this by requiring users annotate their code at
every step, but though this would be a predictable annotation scheme, it would
be extremely burdensome. Using an SMT solver is meant to reduce the tedium of
proving straightforward facts, not just trade it for the tedium of requiring a
type annotation at each step.

At the same time, while it would be great if we could infer all of them, this
is often not necessary and counter-productive in practice because annotations
on top-level functions (most commonly, types) serve as very useful
documentation, which programmers are already used to writing.

Another reason \kl{CN} is quite willing to accept per function annotations is
that it allows checking a large code base to be parallelised, and obviate the
need to do any complex and potentially slow inter-procedural analysis.
Unfortunately, since loops are effectively function tail calls, and invariants
are also difficult to infer, \kl{CN} requires annotation for them too.

Still, pre- and postconditions on functions, and invariants on loops is a crisp
way to explain to programmers where annotations need to be placed. What we
would like to avoid is annotations on each statement of a C program.

More concretely, in the presence of constraints due to refinement types, this
means we would like \emph{check} types at function calls, function returns,
before and after loops from the top down, and \emph{synthesise} types
everywhere else from the bottom up. This is precisely the sort of problem a
bidirectional type system can solve (see sections 4.4 and 4.6
in~\sidetextcite{dunfield2021bidir}). Moreover, it gives a clear principle of
when to use subsumption rule (call the SMT solver): at the boundary between
synthesis and checking (\cref{fig:simple-bidir}).

\begin{figure}
    \begingroup%
    \newcommand{\synths}[1]{\Rightarrow{} \outpol{#1}}
    \newcommand{\checks}[1]{\Leftarrow{} #1}
    \begin{mathpar}
    \begin{tabular}{l@{}l@{ }l@{ }l}
    \cncom{types}       & $T$      & $\mathrel{{\Colon}{=}}$ & $\cnkw{unit} \mid{} T_1 \rightarrow{} T_2$ \\
    \cncom{expressions} & $e$      & $\mathrel{{\Colon}{=}}$ & $x \mid{} \cnkw{()} \mid{} (e {:} T) \mid{} \lambda{} x.\ e \mid{} e_1\ e_2$ \\
    \cncom{context}     & $\Gamma$ & $\mathrel{{\Colon}{=}}$ & $\cdot \mid{} \Gamma{}, x {:} T$ \\
    \\
    \multicolumn{2}{l}{%
        \multirow{2}{*}{%
        \begingroup%
        \setlength{\fboxsep}{2pt}
        \fbox{%
            \shortstack{$\Gamma{} \vdash{} e \synths{T}$ \\ $\Gamma{} \vdash{} e \checks{T}$ }%
        }%
        \endgroup%
        }%
    }
                         & \multicolumn{2}{l}{\cncom{given $\Gamma$, $e$ synthesises $T$}} \\
    \multicolumn{2}{l}{} & \multicolumn{2}{l}{\cncom{given $\Gamma$, $e$ checks $T$}} \\
    \end{tabular}
    \and
    \inferrule[{[Var]}]
        {(x {:} T) \in{} \Gamma}
        { \Gamma{} \vdash{} x \synths{T}}
    \and
    \inferrule[{[Sub]}]
        { \Gamma{} \vdash{} e \synths{T_1} \and T_1 \cnkw{<:} T_2}
        { \Gamma{} \vdash{} e \checks{T_2}}
    \and
    \inferrule[{[Annot]}]
        { \Gamma{} \vdash{} e \checks{T} }
        { \Gamma{} \vdash{} (e {:} T) \synths{T} }
    \and
    \inferrule[{[$\cnkw{unit}$I]}]
        { }
        { \Gamma{} \vdash{} \cnnt{()} \checks{\cnkw{unit}} }
    \and
    \inferrule[{[$\rightarrow{}$I]}]
        { \Gamma{}, x {:} T_1 \vdash{} e \checks{ T_2 } }
        { \Gamma{} \vdash{} \lambda{} x.\ e \checks{ T_1 \rightarrow{} T_2}}
    \and
    \inferrule[{[$\rightarrow{}$E]}]
        { \Gamma{} \vdash{} e_1 \synths{T_1 \rightarrow{} T_2} \and \Gamma{} \vdash{} e_2 \checks{T_2}}
        { \Gamma{} \vdash{} e_1\ e_2 \synths{T_1 \rightarrow{} T_2}}
    \end{mathpar}
    \endgroup%
    \caption{Simple \intro{bidirectional} type system, adapted
    from~\cite{dunfield2021bidir}.  Note that its splits the traditional
    $\Gamma{} \vdash{} e : T$ judgement, into two mutually recursive judgements,
    one for \intro[synthesis]{synthesising} a type and one for checking a type.
    %
    % Here, introduction forms for $\cnkw{unit}$ and $\rightarrow$
    % (namely $\cnkw{()}$ and $\lambda x.\ e$ respectively) check their types;
    % variables, annotated terms and elimination forms for $\rightarrow$ (namely
    % function application $e_1\ e_2$) synthesise their types.
    I highlight synthesised types in a \colorbox{pink!30}{light pink} to make
    them easier to spot.
    %
    If a term is syntactically synthesising, but is used in a checking position,
    then \textsc{Sub} allows it to be checked by first synthesising a type $T_1$
    for it, and then checking whether $T_1 <: T_2$. Here, the subtyping
    relation $\cnkw{<:}$ is merely structural type equality $=$, but in the
    presence of refinement types, it would be logical
    entailment.}\label{fig:simple-bidir}
\end{figure}

\section{Linearity to manage (non-leaky) resources}

Of course, if the aim is define a type system rich enough to prove the absence
of \kl{UB}, we must acknowledge the fact that there are plenty of ways of
getting \kl{UB} which do not rely on pure fragment evaluation rules of the
\kl{standard} being violated, but based on effectful fragment, such as loose
evaluation order or a myriad of subtle memory violations.

In \nameref{sec:sep-logic-intro}, I sketched out how separation logic can be
used to reason about the behaviour of imperative programs which manipulate a
heap and can have syntactically distinct terms \intro{alias} the same location
in said heap. If we want to embed separation logic propositions into a type
system, then we need to be able to precisely control how those propositions are
handled.

Whereas in a typical (intuitionistic) type system, duplicating an assumption in
the context is perfectly acceptable, in a separation logic setting, this would
mean that one could take $x \mapsto{} y \ast{} y = 5$, duplicate the points-to
to get $x \mapsto{} y \ast{} x \mapsto{} y \ast y = 5 \vdash{} x \mapsto{} 0$,
instead of $x \mapsto{} 5$ as we would like. This is because the semantics of
$\ast{}$ means that the context entails $\mathsf{False}$. Such duplication is
known as \intro{contraction} and we need to forbid this to reason soundly
(whilst keeping other assumptions, such as the type of \kl{Core} program
variables, and constraints, contractible).

\begin{marginfigure}
  \begin{mathpar}
      \inferrule[{[Weakening]}]{
          \Delta{}, A \vdash{} C
      }{
          \Delta{} \vdash{} C}
      \\
      \inferrule[{[Contraction]}]{
          \Delta{}, A, A \vdash{} C
      }{
          \Delta{}, A \vdash{} C}
      \\
      \inferrule[{[Exchange]}]{
          \Delta_1, B, A, \Delta_2 \vdash{} C
      }{
          \Delta_1, A, B, \Delta_2 \vdash{} C}
    \\
  \end{mathpar}
  \caption{Substructural sequent calculus rules.}\label{fig:substructural}
\end{marginfigure}

Similarly, in an intuitionistic setting, we can freely forget things we know,
in a separation logic setting this really depends on whether the user of the
logic cares about memory leaks or not. In the C standard, nothing relies on an
allocation being \emph{dead}, it only relies on it being \emph{live}, or not
caring at all. So whilst the theory and the C standard leave this open as a
design choice, the intended use of \kl{CN} in resource constrained settings
requires us to be explicit about when we deallocate and more importantly
\emph{when we forget to}.\sidenote{Though not yet supported by \kl{CN},
concurrency also requires being explicit about destroying a resource, for
example, to prevent a user from acquiring the same mutex twice.} This
`forgetting' an assumption is known as \intro{weakening}, and given the
intended use case we would like to forbid this too (whilst not requiring type
information and ambient constraints to be forgotten freely).

We would however like the ability to commute assumptions freely, since it is
perfectly acceptable to free in any order, regardless of the order of
allocation, so we keep \intro{exchange}. Sequent calculus rules for contraction,
weakening, and exchange are shown in \cref{fig:substructural}. Jettisoning
weakening and contraction but keeping exchange means that we handle our
(heap-related) separation logic assumptions
\emph{linearly}~\sidecite{girard1987linear} in \kl{CN}.

Note that this is in constrast to popular approaches such as the drop trait in
Rust,\sidenote{\url{https://doc.rust-lang.org/std/ops/trait.Drop.html}} or RAII
(Resource Acquisition Is Initialisation) in
C++,\sidenote{\url{https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines\#Re-raii}}.
These allow the programmer to elide explicit resource deallocation, by
automatically inserting such a call when the resources goes out of scope.
Modelling that would require an \kl{affine} rather than linear resources.

Linearity may be necessary, but it can also be cumbersome. If a linear resource
represents the permission to dereference a pointer, a typing rule which
consumes that must also return an identical resource, so that it may be
dereferenced again. If we require the programmer to handle these permissions
explicitly in the annotations, such a discipline would quickly violate the goal
to minimise obvious annotations. This means that in addition to automation for
proving constraints, by using SMT solvers, we need to have some scheme to
\intro[resource inference]{infer resources} (see \cref{sec:elaboration}) to
reduce the burden of annotations.

\begin{marginfigure}
  \begin{mathpar}
      %\inferrule[Declarative Var]{
      %}{
      %    x {:} t \vdash{} x {:} t}
      %\\
      %\inferrule[Algorithmic Var]{
      %}{
      %    \Gamma{} , x {:} t \vdash{}  x {:} t \dashv{} \Gamma'}
      %\\
      %\{\, x {:} t \,\} = \Gamma' - \Gamma
      %\\
      \inferrule[{[Declarative Pair]}]{
          \Delta_1 \vdash{} e_1 {:} T_1
          \and
          \Delta_2 \vdash{} e_2 {:} T_2
      }{
          \Delta_1, \Delta_2 \vdash{} (e_1, e_2) {:} T_1 \times{} T_2}
      \\
      \inferrule[{[Algorithmic Pair]}]{
          \Gamma_1 \vdash{} e_1 {:} T_1 \dashv{} \Gamma_2
          \\
          \Gamma_2 \vdash{} e_2 {:} T_2 \dashv{} \Gamma_3
      }{
          \Gamma_1 \vdash{} (e_1, e_2) {:} T_1 \times{} T_2 \dashv{} \Gamma_3}
      \\
      \Delta_1 = \Gamma_2 - \Gamma_1 \and \Delta_2 = \Gamma_3 - \Gamma_2
  \end{mathpar}
  \caption{Declarative ($\Delta$) versus algorithmic ($\Gamma$) typing for
  linear pairs. A declarative system is easier to describe and
  prove sound, because the recursion is obviously structural, but is not
  directly implementable: for pairs, the correct split of the contexts must be
  guessed before checking each component. In contrast, an algorithmic system
  changes and chains the contexts across premises, which is easy to implement as
  mutating state, but requires a notion of ``context difference'' to relate to
  the declarative version.}\label{fig:decl-alg}
\end{marginfigure}

I should note that, for the purposes of the formalisation of \kl{Kernel CN}, I
do use explicit linear resource terms, since that makes the system easier to
state and prove sound. For clarity and convenience, the linear context in the
system is used in a \intro{declarative} manner (\cref{fig:decl-alg}).
Explicit linear resource terms also makes it easy to specify and explain the
\kl{CN} \kl{resource inference} algorithms as an \emph{elaboration}, which I
will also do.\sidenote{I have not embarked on any proofs about the properties of
these inference algorithms.} Those rules use the linear context in
an algorithmic manner.

\section{Precise assertions for inferring resources and quantifiers without
backtracking}\label{sec:precise-assertion-inferring}

Lurking not too far in the background of the decision to use linear separation
logic and liquid types is a tension between the rich scheme of quantifiers
needed to write specifications expressive enough to prove \kl{UB}-freedom (if
not full functional correctness, where those can be separated) and the desire
to (a) not send quantified formulas to the SMT solver and (b) minimise the
annotations (quantifier instantiations) a user needs to write.

So aside from minimising annotations for proofs of obvious statements (by
carefully using an SMT solver) and minimising annotations for linear resource
terms (by inferring them), we would also, as far as possible, like to
infer how quantifiers are instantiated, based on what the user wrote.\sidenote{
Note that this is not at all related to \emph{inferring specifications}, which
is a large and challenging problem in its own right.} This is a tall order,
because the problem is not decidable in
general~\sidecite{turing1936computable,church1936unsolvable}.

Yet, we can still proceed and make the problem clearer by using a concrete
example. Let us recall the separation logic proof sketch of linked integer list
append from \cref{fig:append-annot,fig:list-pred}, reproduced in
\cref{fig:append-annot-formal,fig:list-pred-formal} for convenience.

\begin{marginfigure}
    \small
    \centering
    \begin{align*}
        \mathrm{list} &(\mathsf{p}, l) \mathrel{{=}^\mathrm{def}} \\
                      &\mathsf{emp} \astRef{} (\mathsf{p} = \mathsf{NULL} \wedge{} l = []) \\
                      &\vee{} \exists{} \; {head}, \; {tl}, \mathsf{p\_tail}.\\
                      &\qquad (\mathsf{p} \mapsto{} {head}) \\
                      &\qquad \astRef{} (\mathsf{p} + 1 \mapsto{} \mathsf{p\_tail}) \\
                      &\qquad \astRef{} \mathrm{list} (\mathsf{p\_tail}, {tl}) \\
                      &\qquad \astRef{} l = {head} {:}{:} {tl} \\
    \end{align*}
    \caption{Definition of a recursive list predicate in a simple separation
        logic.}\label{fig:list-pred-formal}
\end{marginfigure}

\begin{figure}[h]
    \inputminted[breaklines,mathescape,fontsize=\small]{py}{code/append_annot.py}
    \caption{A separation logic proof sketch of a linked integer list
        append.}\label{fig:append-annot-formal}
\end{figure}

We see that in addition to the pointers $\mathsf{xs}$ and $\mathsf{ys}$, which
can be thought of as a quantification over \intro{computational} variables, the
specification for append also quantifies over \emph{logical} or ghost variables
$l_1$ and $l_2$. Given that the ``computational terms'' are just code, the
computational variables need not be inferred, since the user needs to provide
them for the call to the function to be valid.\sidenote{For the rest of this
section, I am only going to talk about quantifiers for function calls, but the
exact same principle applies for dealing with both computational and logical
return values too. I will discuss quantifiers related to array reasoning in
\cref{sec:it-array}.}

However, it would be a shame if the user had to provide explicit instantiations
for $l_1$ and $l_2$ at each call site, including the recursive call within the
implementation of \mintinline{py}{append}. If we want to stick to a liquid
typing discipline, where every quantifier must be matched up to a program
variable, we need a way to bind the ghost values which were existentially
quantified over in the $\mathrm{list}$ predicate of
\cref{fig:list-pred-formal}, to names the user would have to choose.

This seems even more of a tragedy because before the recursive function
call, we already know (a) $\mathrm{list}(\mathsf{xs}', l_1')$ and
$\mathrm{list}(\mathsf{ys}', l_2')$ and (b) the precondition requires choices
of $l_1$ and $l_2$ such that $\mathrm{list}(\mathsf{xs}', l_1)$ and
$\mathrm{list}(\mathsf{ys}', l_1)$ so there is \emph{only one sensible choice}
for those quantifiers.

To recap, when calling a function, we need some way of guessing some
instantiation of quantifiers such that it satisfies the precondition. \emph{If}
we are in a situation where there is only one sensible instantiation given the
required and the available assertions, a simple inference scheme would be to
simply scan the context for predicates which match on constructor (i.e.\
$\mathsf{emp}$ matches with $\mathsf{emp}$, $\_ \mapsto{} \_$ matches $\_
\mapsto{} \_$, $\mathrm{list(\_, \_)}$ matches $\mathrm{list}(\_, \_)$) and
\emph{computational} arguments, and instantiate any quantifiers based on the
remaining values.

So in this instance, when calling \mintinline{py}{append(xs->tail, ys)}, % chktex 36
such an inference scheme would
\begin{enumerate}
    \item Delay instantiating $l_1$ and $l_2$.
    \item Note that the precondition requires $\mathrm{list}(\mathsf{xs}',
        l_1)$ and $\mathrm{list}(\mathsf{ys}', l_2)$.
    \item Check the context for a match with $\mathrm{list}(\mathsf{xs}',
        \_)$.\sidenote{Checking whether two symbolic terms are equal can be
        automated with an SMT solver.}
    \item Find $\mathrm{list}(\mathsf{xs}', l_1')$.
    \item Select $[l_1' / l_1]$ as its instantiation.
    \item Repeat similarly for $\mathsf{ys}$ and $l_2'$.
\end{enumerate}

It is a cute idea, but it glosses over several details, such as
\begin{itemize}
    \item Are such assertions characterisable, and if so, how?
    \item Can such a scheme handle disjunction, $ P \vee{} Q$?
    \item Are assertions where \kl{computational} arguments uniquely determine
        \kl{logical} arguments, expressive enough for realistic code?
    \item What is the correct atomic predicate to look up in a context?
    \item Can all assertions be decomposed into a context of such atomic
        predicates?
    \item Should it unfold recursive predicates, and if so, when?
\end{itemize}

I will tackle the first three questions here, and leave the latter three
to be discussed in~\nameref{sec:heap-types}.

For the first question, the answer is: in practice, \emph{yes}. If one assigns
input and output \kl{modes} to the arguments of separation logic predicates,
and ensures that predicates are \emph{mode-correct} via a syntactic check
(\nameref{sec:monadic-syntax}), then that suffices to guarantee that the system
can \emph{always infer quantifiers}.

However, the theoretical justification for this unclear. Such assertions happen
to be characterisable as \kl{precise}~\sidecite{reynolds2008intro}, but the
connection to modes has not been formally explained.

\subsection{Precise assertions}\label{subsec:precise-assertion}

\begin{definition}[Precise assertions]%
\label{def:precise-assertion}
    \AP{} An assertion $Q$ is \intro{precise} iff, for all stores $s$, and heaps
    $h$, there is at most one $h' \subseteq{} h$ such that $h' \in [\![ Q ]\!] (s)$.

    In other words, if a \kl{precise} assertion holds on a subheap, then it
    does so uniquely.
\end{definition}

Based on this definition, one can derive some examples and properties of
precise assertions as shown in~\cref{fig:precise} (examples of imprecise
assertions are shown in~\cref{fig:imprecise}). As I shall show later, each
production in the grammar for \kl{linear} \kl{resource types}
(\cref{fig:kernel-res}), stays within the precise fragment.

\begin{marginfigure}
\begin{mathpar}
    \mathsf{emp}
    \and p \mapsto v
    \and \exists v.\ p \mapsto v
    \and \mathrm{list}(\mathsf{p}, l)
    \and \exists l.\ \mathrm{list}(\mathsf{p}, l)
    \and P \wedge Q \text{ when $P$ or $Q$ is precise}
    \and P \ast Q \text{ when $P$ and $Q$ are precise}
\end{mathpar}%
\caption{Some examples and properties of precise assertions.}\label{fig:precise}
\end{marginfigure}

\citeauthor{brotherston2016model}~\sidecite{brotherston2016model}~showed that
precise assertions allow for efficient model checking without backtracking;
in~\nameref{sec:elaboration}, I show how the same allow for \kl{CN} to infer
instantiations of quantifiers without backtracking. I do so by leaning on input
and output \kl{modes} which (a) is intuitive, given that in C source a pointer
is an ``input'' and dereferenced value an ``output'' and (b) happens to
coincide.

This brings us nicely to the second question about disjunctions. As visible
from \cref{fig:imprecise}, we see that arbitrary disjunctions pose a problem.
Yet, from \cref{fig:precise}, we see that the list predicate, and even a
version of it which is existentially quantified over its ghost list fits the
definition of precise, despite including a disjunction in its definition
(\cref{fig:list-pred-formal}).

\begin{marginfigure}
\begin{mathpar}
    \mathsf{true}
    \and \mathsf{emp} \vee{} \mathsf{x} \mapsto{} 42
    \and \mathsf{x} \mapsto{} 3 \vee{} \mathsf{y} \mapsto{} 7
    \and \exists \mathsf{p}.\ \mathsf{p} \mapsto{} 1
    \and \exists \mathsf{p}.\ \mathrm{list}(\mathsf{p}, l).
\end{mathpar}
\caption{Examples of im\kl{precise} assertions.}\label{fig:imprecise}
\end{marginfigure}

This is because the two arms of the disjunction make assertions
which are disjoint on \emph{both} the values the computational parameter
$\mathsf{p}$ can take \emph{and} on the shape of the heap; one branch says
$\mathsf{p} = \mathsf{NULL}$ and the heap will be $\mathsf{empty}$, whereas the
other branch guarantees that (implicitly) $\mathsf{p} \neq \mathsf{NULL}$ and
that the heap will have \emph{at least two} heap cells. I will return to the
discussion of disjunctions in precise predicates imminently; for now I want to
note that the locations of those two cells are expressed entirely in terms of
program variables.

At this point, a keen reader may have noticed that while this may be true at
the `top-level' of the predicate, the recursive call does use the existentially
quantified ghost value $\mathsf{p\_tail}$, which is on the right of $\mathsf{p}
+ 1 \mapsto{} \mathsf{p\_tail}$. So why is this definition still \kl{precise}?
It is because in any heap which satisfies the non-empty branch of the list
predicate, $\mathsf{p\_tail}$ is \emph{uniquely} determined by its relation as
the value in the location adjacent to $\mathsf{p}$.\sidenote{A proof of this is
left as an exercise to the reader.} This can be operationalised with a \kl{mode}
discipline for predicate arguments, which I shall discuss in
\nameref{sec:monadic-syntax}.

Finally, the third question is an empirical one, but our experience using
\kl{CN} so far~\sidecite{pulte2023cn,pulte2024tutorial} suggests the answer is
yes, it is sufficient for small to medium tutorial examples, all the way up to
specifying the buddy allocator (\cref{chap:buddy}) used in pKVM\@.

That being said, we have found that being able to abstract over, and supply
quantifiers manually can sometimes be useful for ergonomics, even if not
strictly necessary. An example came from the efforts of Cassia Torczon to
verify a basic string manipulation library in \kl{CN}.%
\sidenote{\href{https://github.com/rems-project/cerberus/issues/540}{see
Cerberus\#540}} The natural representation of null-terminated strings, as
required by many of the library functions, is a recursive predicate. However,
the implementation of such library functions use while-loops and
array-indexing, for which it is convenient (but not neccessary) for proof (and
library clients) to express the string as an array (represented by an iterated
predicate), over a finite range. The end of that range is fixed, but not
present before looping and indexing in the program code. Hence to refer to it
in the specification, we need the ability to abstract over and supply
quantifiers manually.

\section{Monadic syntax to mode-correctness}\label{sec:monadic-syntax}

\intro[mode]{Modes} help us ensure that we can infer quantifiers instantiations
as follows. We start with separation logic predicates, and label their
arguments as either $\mathsf{in}$puts or $\mathsf{out}$puts, as shown in
\cref{fig:mode-pred}. In the scope of each quantifier, for each predicate, we
check the mode of each parameter matches the mode of each argument.
Computational arguments the user provides are considered inputs. If a
quantified variable is used in an input position, it must also be used
elsewhere in an output position.

\begin{marginfigure}
    \centering
    \begin{align*}
        \_ \mapsto{} \_ \:&: \mathrm{Loc}^{\mathsf{in}} \times \mathrm{Value}^{\mathsf{out}} \rightarrow \mathrm{Prop} \\
        \mathrm{list} \:&: \mathrm{Loc}^{\mathsf{in}} \times \mathrm{IntList}^{\mathsf{out}} \rightarrow \mathrm{Prop}
    \end{align*}
    \caption{Modes on separation logic predicates.}\label{fig:mode-pred}
\end{marginfigure}

So for example, the second disjunct of \cref{fig:list-pred-formal}, the
quantified variable $\mathsf{p\_tail}$, occurs in $\mathrm{list}
(\mathsf{p\_tail}, {tl})$ in the input position, but this is fine because it also
occurs in the output position in $\mathsf{p} + 1 \mapsto{} \mathsf{p\_tail}$.
However, in the example from \cref{fig:imprecise}, $\exists \mathsf{xs}. \
\mathrm{list}(\mathsf{xs},l)$ is badly moded because $\mathsf{xs}$ occurs in an
input position, but nowhere else in an output.

Ensuring that quantifiers preserve \kl[precise]{precision} in assertions by
dividing up predicate arguments into input and output modes is
not new~\sidecite{somogyi1996execution,berdine2006smallfoot,nguyen2008runtime,maksimovic2021gillian},
including for the purposes of reducing the burden of
annotation~\sidecite{jacobs2011verifast}.

What is new is the target audience of kernel programmers, rather than
verification specialists. Programmers may not be used to thinking in terms of
ghost state, and discussions with our users suggest that adding quantifiers and
\kl{mode}s on top of that would be step too far.

An elegant solution to both inscrutable mode rules, and the problem of
disjunctions mentioned earlier, is to change the perspective on
\cref{fig:mode-pred}~\sidecite{krishnaswami2022monadic}. A predicate
$\mathrm{list} : \mathrm{Loc} \times \mathrm{IntList} \rightarrow
\mathrm{Prop}$, considered set-theoretically is just $\mathcal{P} (\mathrm{Loc}
\times \mathrm{IntList})$, which is isomorphic to $\mathrm{Loc} \rightarrow
\mathcal{P} (\mathrm{IntList})$. Unlike the initial form, which gave ``equal
weighting'' to input and outputs, this form makes the input role of
$\mathrm{Loc}$ and the output role of $\mathrm{IntList}$ very natural.

\begin{marginfigure}
    \begin{align*}
        &\llbracket \_ \rrbracket \::\: \mathcal{P}(\tau) \rightarrow (\tau \rightarrow \mathsf{Prop}) \\
        &\llbracket \mathsf{return}\ t \rrbracket = \lambda a.\ t = a \wedge{} \mathsf{emp} \\
        &\llbracket \mathsf{Own(t)} \rrbracket = \lambda a.\ t \mapsto{} a \\
        &\llbracket \mathsf{let\ x} = e; e' \rrbracket \\
        &\qquad = \lambda a.\ \exists x.\ \llbracket e \rrbracket x \astRef{} \llbracket e' \rrbracket a \\
        &\llbracket \mathsf{if}\ t\ \mathsf{then}\ e\ \mathsf{else}\ e' \rrbracket \\
        &\qquad = \lambda a.\  \left( t \wedge \llbracket e \rrbracket a \right) \vee \left( \neg\;t \wedge \llbracket e' \rrbracket a \right)
    \end{align*}
    \caption{Monadic syntax for separation logic, along with a translation into the traditional presentations. Pure
        terms are denoted by $t$, and monadic expression are denoted with $e$.}\label{fig:monad-sl}
\end{marginfigure}

Not only does this notational transformation turn \emph{mode correctness} into
\emph{variable scoping} with the $\mathsf{let\ x} = e; e'$ construct, it also
enforces the disjointness of computational values and the shape of the heap
mentioned earlier with the $\mathsf{if}\ t\ \mathsf{then}\ e\ \mathsf{else}\
e'$ construct. Not only can all logical quantifiers now be \emph{inferred}
without backtracking, branching assertions can be checked without backtracking
too: check $e$ if you can can prove $t$ and check $e'$ if you can prove
$\neg\;t$.

\subsection{\kl[iterated]{Iterated} separating conjunctions to handle arrays}\label{sec:it-array}

C programmers use arrays a lot, particularly with computed index (i.e.\
`random') access, rather than following a particular traversal pattern to get
some element in the middle.

Typically (such as in VeriFast) arrays are handled via recursive predicates,
and if the order of traversal in C does not match the order of traversal in the
predicate, or access is required to the middle of the array, then a user needs
to use lemmas of some sort to massage the available assertions into a usable
form.

\kl{CN} implements \intro{iterated separating conjunctions}, inspired by
\sidetextcite{muller2016automatic}, but with restrictions to ensure only
quantifier-free SMT queries. Specifically,
\begin{itemize}
    \item Iterations must take the form
        \cninline[breaklines]|each (<type> i; <guard>) { <pred>( array_shift(p i) ) }|, % chktex 36 chktex 37
        where
        \begin{itemize}
            \item \cninline{i} is the iterating index being quantified over
            \item \cninline{<guard>} is a boolean condition on \cninline{i}
            \item \cninline{p} is a base pointer
            \item \cninline{<pred>} is the name of a named predicate
            \item \cninline{array_shift} is the specification language syntax
                for C's \cinline{p + i}.
        \end{itemize}
    \item Indices which are intended to move into or out of an iterated separating
        conjunction must be declared so explicitly with a \cninline{focus <idx>;}
        statement.
\end{itemize}

This enables a few convenient features:
\begin{itemize}
    \item Iterated separating conjunctions can be tested for equality by
        looking only at the name of the predicate being iterated over, symbolic
        equality of the base pointer, and symbolic equivalence of the guard
        conditions.
    \item Predicates can be moved out of and into iterated separating conjunctions
        automatically.
    \item Iterated separating conjunctions can be split along an arbitrary
        guard automatically in a similar fashion, except `subtraction' is done as follows:
        for guard \cninline{A = 0i32 <= i && i < 10i32} and guard
        \cninline[breaklines]{B = A && mod(i, 2i32) == 0i32}, % chktex 36
        the subtraction is simply \cninline{A && !B}. Note that the split % chktex 26
        does not have to be contiguous.
\end{itemize}

\kl{CN} does not however, support automatically \emph{merging} arrays, because
the constraints required to express the \emph{merged values} are outside the
decidable fragment for the SMT theory of arrays.\sidenote{So \kl{CN} could, but
does not, support merging for uninitialised arrays.} Relatedly, \kl{CN} did
support the decidable array property fragment,~\sidecite{bradley2006whats}, but
it was not expressive enough for verifying pKVM's \kl{buddy allocator}
(\cref{chap:buddy}) and so it was removed. \kl{CN} also supported a scheme for
inferring indices (instead of requiring the movable ones to be declared up
front), but this was removed during a change to the inference
scheme.\sidenote{\href{https://github.com/rems-project/cerberus/commit/7c2c0a364a4373e4eb109f32d01cc9584f51e81f}{Commit
7c2c0a36.}}\label{sn:new-inf}

\section{Alternatives and related work}

\begin{itemize}
    \item Do I want to talk about this here?
\end{itemize}

