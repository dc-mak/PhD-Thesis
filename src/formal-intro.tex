In the \cref{sec:c-lang}, I discussed how competing forces of inherited
portability requirements, proximity to hardware, and the desire for more
aggressive optimisations led to complex and subtle technical resolution by
stakeholders in the \kl{ISO} standard of C. I also mentioned that its nature as
a prose document, with natural language ambiguities and omissions, as well as
divergence C as used \kl{de facto}, mean that its semantics are unreasonable
for a human to adhere to, and challenging to build into tools directly,
without making some sort of simplifying assumptions.

Existing program logic frameworks for C such as Verifiable C~\sidecite{appelSF5}
and RefinedC~\sidecite{sammler2021refinedc} take the approach of building a
logic directly above an operational semantics for a language which is
recognisably C, minus some desugaring to consolidate similar constructs. They
attempt to retain as many C features (control flow, variable scoping, aliasing,
loose evaluation order, pointer manipulation rules) as possible, but make
simplifying assumptions where it would be impractical otherwise.

Given that one of \kl{CN}'s goals (\cref{sec:cn-intro}) is to work with
pre-existing C programs, which rely on many if not all of those impractical
features, adopting the conventional approach would quickly use up most of its
complexity budget and make the other goal (of reducing the expertise required
to do verification) unfeasible.

Instead, \kl{CN} builds directly upon the
\kl{Cerberus}~\sidecite{memarian2022cerberus} executable and empirically
validated semantics for C. Not only does \kl{CN} benefit from the
\emph{accurate} semantics for both \kl{ISO} and \kl{de facto} C, it benefits
most from the \emph{usability} of it. This is because, Cerberus is elaborated
into a relatively small calculus \emph{\kl{Core}}, which translates all of C's
complexity into a first-order functional language with a few special (but easy
to understand and specify) constructs.

Additionally, \kl{CN} is intended to be used more like a \emph{type system} in
an IDE than a program logic inside a proof assistant. Ideally, instead of
seeing intermediate goals in a sophisticated separation logic, and needing to
be well versed with a range of inference rules and automation tactics, a user
sees their C program, scattered with predictable and lightweight annotations in
comments, in an editor which either indicates success, or clear and helpful
error message.

Aside from the fact that the notion and mode of use of a type system is more
familiar to most programmers (an advantage not to be scoffed at), this approach
also allows \kl{CN} to use and advance the extant literature on building
refinement type systems on top of existing languages.

This type system approach also leads to other desiderata and their
corresponding responses. If we want to follow a type system approach, we want
to minimise obvious annotations, so need some sort of automation so as to not
burden the programmer with proving things like $1 + 1 = 2$. Similar to
VeriFast~\sidecite{jacobs2011verifast} and Frama-C\sidecite{kirchner2015frama},
\kl{CN} enlists the support of SMT solvers to mitigate this. When trying to
verify code against expressive specifications, this could lead to
non-termination, so \kl{CN} also restricts the expressiveness of the assertion
language, and the queries it sends to the SMT solver. And given the importance
of managing resources in C, the typing discipline needs to be substructural.

The \kl{CN} assertion language syntax aims to be expressive enough to verify
real world C, but also restricted enough to limit the aforementioned technical
problems, and intuitive enough to a target audience of systems programmers who
happen to know Haskell (or Rust).

With this many constraints and design decisions, it is easy to doubtful of the
elegance and feasibility of this approach, let alone consider proving such a
type system sound. As I will show in \nameref{chap:kernel-cn}, whilst the setup
might be novel, multi-faceted and large, the definitions are relatively
straightforward, and the proof of soundness can be done syntactically. Both the
definitions and the proof are modular with respect to the heap, so that
changing the memory object model does not require redoing the entire soundness
proof. The formalisation is close enough to the surface syntax of \kl{CN} so
that a correspondence between the two can be stated simply and precisely, and
close enough to the implementation to offer actionable insights.

\chapter{Formalisation Background}%
\label{chap:formal-background}

\section{Cerberus and Core}%
\label{sec:cerberus-core}

An empirically validated, compositional elaboration semantics from C to Core.
Why does this matter \textemdash{} for placing user-annotation into refinement types
easily at the appropriate program points!

Relatively small, simpler, variables that make sense (but this bites later because SMT error messages).

Compositional elab, annotations.

Example (list append) and example in the Core.

\section{Refinement Types}

What are refinement types? Why do we care about decidability? It's a short hand
for usable, SMT, imperfect proxy for performance, to be discussed later.

You can layer over the existing system (from Kayvan's thesis), \textendash{}
rule for undef means (previously YOLO) to context has to be false. E\.g\. same if-then-else twice, so dead code.

Liquid types.

\subsection{Friendly and convenient syntax}\label{sec:friendly-syntax}
Separation logic types, syntax and restrictions for it.

Explicit witness to having permissions, which are linearly typed (just ingredients).

\chapter{Kernel CN:\ A Bidirectional, Separation Logic Refinement Type System for Core}%
\label{chap:kernel-cn}

This will be a lot of pages.
Explain \intro{bidirectional} (for quantifiers), linear resources, constraints.
Explain enough rules \textemdash{} typing, operations, especially the weird heaps.
And of course, type safety statement and its proof.

\subsection{Resource Terms, Quantifier Inference}

Linear terms in a refinement type system.

(Dep ML, L3, F star, Jhala, ATS)

Different and unusual compared to Iris style \textemdash{} separation proofs outside the program.

Proof term in one sense, but also factors out operations for resource manipulation.

\subsection{Heaps}


\subsection{Type Safety}

\begin{comment}

When presenting a typing derivation the way we did in \cref{chap:tech-intro}, there is
an important piece of information missing.
In logical programming, this is called the \kl{mode} of the inference rules,
\ie which objects are considered as inputs and which as outputs in the search
for a derivation.
This information, however, is crucial when one tries to build a type-checker:
some rules might seem fine when writing them down on paper, but trying to give them a
sensible mode fails, indicating they are not suited for an implementation.
In the case of the typing judgement $\Gamma \vdash t \ty T$,
usually both the term $t$ under inspection and the context $\Gamma$ are inputs –
although some depart from this \sidecite{Jim1996}.
The mode of the type $T$, however, is much less clear: should it be inferred based upon
$\Gamma$ and $t$, or do we merely want to check whether $t$ conforms to a given $T$?
Both are sensible questions, and in fact typing algorithms for complex type systems usually 
alternate between them during the inspection of a single term/program.
\AP The bidirectional approach makes this difference between modes explicit,
by decomposing \intro{undirected typing}%
\sidenote{We call anything related to the $\Gamma \vdash t \ty T$ judgement
\kl(typ){undirected}, by contrast with bidirectional typing.}
$\Gamma \vdash t \ty T$ into two separate judgements $\inferty{\Gamma}{t}{T}$
(\kl{inference}) and $\checkty{\Gamma}{t}{T}$ (\kl{checking})%
\sidenote{We use the $\ity$ and $\cty$ symbols rather than the more usual
$\Rightarrow$ and $\Leftarrow$ to avoid confusion with implication and with the
\kl{Coq} notation for functions.},
that differ only by their \kl{modes}. The type is an
\kl{input} in inference, but an \kl{output} in checking.
Following this decomposition%
\sidenote{Pioneered by \textcite{Pierce2000}, a general survey can be found
in \textcite{Dunfield2021}.}%
\margincite{Pierce2000}%
\margincite{Dunfield2021}
leads to type systems that are more structured and directly amenable to implementations,
and to good quality algorithms.%
\sidenote{\textcite{Pierce2000} for instance stress good error reporting as an important 
property of their approach.}

This is appealing, but in the dependently typed world, despite advocacy by \eg
\sidetextcite{McBride2018,McBride2019} to adopt this approach during
the design of type systems on paper rather than in their implementations only,
most of the theoretical work to this day remains undirected.
Bidirectionality appears mostly
in articles concentrating on the description of typing algorithms, for instance
\sidetextcite{Huet1989}, \sidetextcite{Coquand1996}, or \sidetextcite{Norell2007}.
However, since these primarily insist on the algorithmic aspect, they do not consider the
bidirectional structure for itself. Moreover, in the case of
\textcite{Coquand1996} and \textcite{Norell2007}, they concentrate on bidirectional typing
as a way to remedy for the lack of annotations on their \kl{Curry-style} λ-abstractions.
This is sensible when looking for lightness of the input syntax, but poses an inherent completeness problem: a term such as $(\l x.\ x)~\z$ is not typeable in those systems.%
\sidenote{They are actually only able to give types to normal forms.}
In the context of \kl{Church-style} abstraction, the closest there is to a description of
bidirectional typing for \kl{CIC} is probably the one given by the
\kl{Matita} team \sidecite{Asperti2012},
which however concentrates again on the challenges posed by the
elaboration and unification algorithms.
They also do not consider the problem of completeness with respect to a given undirected system, as it would fail in their setting due to the undecidability of higher order unification.

In this part (\nameref{part:bidir}), we wish to fill this gap in the literature,
by describing a bidirectional type system that is complete with respect to (undirected)
\kl{CIC}, as presented in \cref{chap:tech-intro}.
By completeness, we mean that any term that is typeable in the undirected system should also
infer a type in the bidirectional one.
This feature is very desirable when implementing kernels for proof assistants,
whose algorithms should correspond to their undirected specification –
even on terms not in normal form. Indeed, reduction is only normalizing
on well-typed terms, so it should not be called on a term that is not known to be
well-typed. Thus if a developer wishes to generate a term using tactics, they cannot use
reduction before knowing that it is well-typed, but might not be able to type-check it
because it is not a normal form… And ensuring that a tactic returns normal forms only
might be unfeasable, and should not be a concern.

The bidirectional system we present naturally forms an intermediate
step between actual algorithms and undirected type systems, something we exploit
in \arefpart{metacoq}.
But its interest is not limited to the relation with implementations.
Indeed, the structure of a bidirectional derivation is more constrained than that of
an undirected one, especially controlling the usage of computation – \eg the conversion rule
\refname{rule:cic-conv}.
This finer structure can make proofs easier,
while the equivalence ensures that they can be transported to the undirected world.
We show this by providing straightforward proofs of \kl{uniqueness of types up to}
\kl{cumulativity}, and of \kl{strengthening}.

% The bidirectional structure also provides a better base for new type systems. This was actually the starting point for this investigation: in \cite{LennonBertrand2020}, we quickly describe a bidirectional variant of CIC, as the usual undirected CIC is unfit for the gradual extension we envision due to the too high flexibility of a free-standing conversion rule. This is the system we wish to thoroughly describe and investigate here.
 
% This step has proven useful in an ongoing completeness proof of MetaCoq's \cite{Sozeau2020a} type-checking algorithm\footnote{A completeness bug in that algorithm – also present in the Coq kernel – has already been found, see \cref{sec:to-pcuic} for details.}: rather than proving the algorithm complete directly, the idea is to prove it equivalent to the bidirectional type system, separating the implementation problems from the ones regarding the bidirectional structure.

As we did in \cref{chap:tech-intro}, we start by exposing the main ideas in the
simpler setting of \kl{CCω}, in \cref{chap:bidir-ccw}.
With those set clear, we go on with their adaptation to \kl{PCUIC}, and the subtle
issues that arose in that context, in \cref{chap:bidir-pcuic}.
Finally, \cref{chap:bidir-conv} describes early investigations into giving a
bidirectional treatment not only of typing, but also of \kl{conversion}.

% This equivalence has been formalised on top of MetaCoq \cite{Sozeau2020}\footnote{A version frozen as described in this article is available in the following git branch: \url{https://github.com/MevenBertrand/metacoq/tree/itp-artefact}.}
% We next turn back to less technical considerations, as we believe that the bidirectional structure is of general theoretical interest. \Cref{sec:beyond} thus describes the value of basing type systems on the bidirectional system directly rather than on the equivalent undirected one. Finally \cref{sec:related} investigates related work, and in particular tries and identify the implicit presence of constrained inference in various earlier articles, showing how making it explicit clarifies those.

\end{comment}
